{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPzi799o7Ghx9uzHVOgftiM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinopiaggi/summarize/blob/main/summarize_videos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using https://huggingface.co/facebook/bart-large-cnn"
      ],
      "metadata": {
        "id": "4QOu6v4AuX21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install tensorflow==2.1\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "f_OFp3uMzB9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29bf4601-679b-48cd-dee6-8736b93b0991"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.1 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vK-jCVHOtE1g"
      },
      "outputs": [],
      "source": [
        "DOCUMENT = \"\"\"\n",
        " And  we  go  Last  lesson  we  end  up  with  vector  clock,  right?  We  arrived  up  to  this  point.  Okay,  so  vector  clock  It's  a  vector  clock  defines  a  perfect  isomorphism  With  respect  to  the  happens  before  relationship.\n",
        " So  lamp  for  clock  defined  in  an  ordering  That  was  stronger  than  the  happens  before  relationship.  What  is  that  vector  clock?\n",
        " Defines  an  ordering  that  perfectly  reflects  that  happens  before  relationship  So  this  is  the  ordering  defined  for  vector  clock  The  vectors  are  equal  if  all  the  positions  are  equal  one  is  less  than  the  other  if  all  position  are  less  than  the  other  One  is  strictly  less  than  the  other  if  it  is  less  than  the  other  and  at  least  one  position  differs  and  then  the  two  vector  clock  are  parallel  if  neither  the  first  is  Proceeds\n",
        " the  second  nor  vice  versa.  Okay,  so  with  this  definition  of  ordering  if  e  happens  before  it  prime  Then  the  vector  of  v  is  lower  strictly  lower  than  the  vector  of  v  prime  and  vice  versa  So  the  important  part  here  is  that  only  also  the  vice  versa  is  true  And  the  second  important  part  here  is  that  now  we  have  a  way  to  check  if  Twins  are  parallel  in  the  happens  before  relationship  Because  if  e  and  e  prime  are  parallel\n",
        " in  the  happens  before  relationship  Then  the  vector  of  e  is  parallel  to  the  vector  of  e  prime  and  vice  versa  Okay,  here  is  an  example  Just  remember  you  how  vector  clock  works  so  vector  clocks  are  Vectors  arrays  of  numbers  The  size  equals  the  size  of  the  processes  so  three  processes  three  elements  in  the  array  in  the  vector  vector.\n",
        " Each  process  has  its  own  position.  So  first  position  is  the  position  of  process  one,  second  position  is  the  position  of  process  two,  etc.  When  an  event  happens  at  a  process,\n",
        " it  increments  its  own  position.  So  the  event  a  happened  at  p1.  So  the  vector  clock  of  p1  becomes  1,\n",
        " 0,  0.  It  was  0,  0,  0  and  now  it  becomes  1,  0,  0.  Because  it  incremented  the  first  position.  The  event  of  sending  messages  is  an  event.\n",
        " So  when  you  send  the  message,  this  is  an  event.  So  the  vector  clock  of  p1  becomes  2,  0,  0.  The  vector  clock  is  put  into  the  message  and  the  message  is  sent.\n",
        " As  usual,  the  message  is  the  application  message.  The  vector  clock  is  put  on  top  of  this  application  message.  So  there  is  always  the  payload  of  the  application  plus  the  vector  clock.\n",
        " Now  the  vector  clock  reaches  the  message,  reaches  process  two.  At  process  two,  the  vector  clock  was  0,  0,  0.  Because  at  the  beginning,  we  assumed  that  we  start  with  0,\n",
        " 0,  0.  Now  p2  receives  the  message.  When  the  message  is  received,  the  vector  clock  of  the  message  and  the  vector  clock  of  the  receiver  are  merged  together,\n",
        " which  means  that  at  every  position  we  take  the  maximum.  And  then  the  receiver  increments  its  own  position.  So  2,  0,  0  merged  with  0,\n",
        " 0,  0  becomes  2,  0,  0.  But  then  there  is  the  increment  because  process  two  received  something.  so  this  is  an  event,  so  we  increment  the  second  position,\n",
        " so  P2  increments  the  second  position  of  its  own  vector  clock  and  the  vector  clock  of  P2  becomes  210.  In  the  meanwhile  at  P3  there  was  an  event,\n",
        " a  local  event,  so  P3  updates  its  own  vector  clock  that  goes  from  000  to  001,  again  P3  as  it  saw  each  process  as  its  own  copy  of  the  vector  clock,\n",
        " P3  increments  its  own  position  because  this  is  a  local  event,  now  there  is  an  event  at  P2,  there  is  an  event  of  sending  a  message,\n",
        " so  P2  increments  its  own  position  from  210  to  220,  then  the  message  is  sent,  the  message  is  received,  001  merged  with  220  becomes  221  plus  the  increment  of  the  receiver  222.\n",
        " Okay,  everything  is  clear,  and  the  same  is  true,  so  notice,  now  let's  check  that  this  is  true,\n",
        " it  is  exact,  so  that  these  two  equivalents  are  true,  consider  those  two  events,  A  and  E,  A  and  E  are  parallel  in  the  act  as  before  relationship,\n",
        " remember,  so  two  events  are  in  the  act  as  before  relationship,  if  there  is  a  chain  of  communication  that  connects  the  first  event  with  the  second  event,  A  and  E  there  is  no  such  chain,\n",
        " so  A  and  E  are  parallel  in  the  act  as  before  relationship,  and  if  you  look  at  the  vector  clock  of  A  and  the  the  algorithm,\n",
        " the  vector  clock  are  this  one.  Is  this  possible?  So  something  has  happened  between  those  three  processes.\n",
        " They  communicated.  They  updated  their  vector  clock.  At  the  end,  this  is  the  result.  Is  this  result  possible?\n",
        " Why?  [INAUDIBLE]  OK,  that's  perfect.\n",
        " So  each  position  of  a  vector  clock  represents  the  vision  of  that  process  regarding  itself  or  regarding  the  other  processes.\n",
        " So  the  first  position  of  the  vector  clock  of  process  one  represents  the  view  of  process  one  of  how  many  events  happen  at  process  one.  The  second  position  of  the  vector  clock  of  process  one  represents  the  view  of  process  one  about  what  happened  at  process  two.\n",
        " In  the  third  position,  the  view  of  process  one  about  what  happened  at  process  three.  And  the  same  is  true  here.  So  the  first  position  of  this  vector  clock  represents  the  view  that  process  two  has  about  what  happened  at  process  one.\n",
        " So  process  two  think  that  five  events  happen  at  process  one.  But  process  one  knows  and  knows  because  they  are  easy  events.\n",
        " That  only  four  events  happen  at  one.  And  this  is  not  possible.  Also  because  the  only  way  by  which  process  2  incremented  this  value  is  because  it  received  something  from  process  1  or  it  received  a  vector  clock  from  another  process  that  before  received  a  vector  clock  from  process  1.\n",
        " This  is  the  only  way  because  you  at  the  other  position  you  always  take  the  maximum  of  what  counts.  At  your  own  position  you  increment,  but  the  other  position  you  take  the  maximum.\n",
        " So  in  order  for  p2  heading  this  five  here  means  that  it  received  this  five  in  some  message.  It  could  be  a  message  coming  from  p1,\n",
        " but  this  is  possible  because  p1  knows  4,  not  5,  or  it  could  be  a  message  coming  from  p3,  but  this  means  that  p3  should  have  received  something  from  p1  that  incremented  the  first  position  up  to  5.\n",
        " So  directly  or  indirectly  this  means  that  p2  received  a  message,  a  vector  clock  from  p1,  where  at  first  position  that  was  5.\n",
        " And  this  is  impossible  because  we  know  that  p1  only  knows  4.  Okay,\n",
        " how  can  we  use  vector  clock  in  practice?  Practical  use  of  vector  clock  is  in  order  to  implement  causal  delivery.\n",
        " We  said  that  the  vector  clock  perfectly  reflects  the  happens  before  relationship.  And  when  we  introduced  the  happens  before  relationship  we  said  the  happens  before  relationship  is  the  best  possible  representation  that  we  have,\n",
        " approximation  that  we  have  about  the  causality  between  events.  So  the  vector  clock  perfectly  reflects  the  best  possible  approximation  that  we  have  about  causality.\n",
        " causality.  So  we  can  use  them  to  order  events  according  to  causality,  not  exactly  causality,  but  the  best  approximation  that  we  have  about  causality.\n",
        " And  in  particular,  in  order  to  do  so,  we  need  a  variation  of  vector  clock.  So  let  me  do  an  example,  imagine  that  we  have  a  bulletin  board,\n",
        " so  something  like  a  group  communication  system,  in  which  messages  and  replies,  messages  and  replies,  questions  and  replies  are  sent  using  reliable  5 -order  channels  to  all  the  boards  in  parallel.\n",
        " So  I  send  a  message  and  I  send  to  all  of  you  in  parallel,  you  receive  it  and  you  send  your  reply  to  all  of  us  in  parallel.  So  I'm  speaking  to  all  of  you,\n",
        " you  make  questions  to  me  and  this  question  is  common,  I  reply  and  this  is  common  and  so  on  and  so  forth.  Now,\n",
        " if  we  start  chatting  together,  those  two  messages  are  parallel,  so  we  don't  want  to  order  them  because  they  are  parallel.  There  is  no  causal  relationship  between  the  first  and  the  second.\n",
        " But  if  you  make  me  a  question  and  I  reply,  all  of  us  would  like  to  hear  the  question  before  the  reply,  the  typical  example.\n",
        " So  we  want  to  order  questions  and  replies,  messages  in  causal  order,  not  in  a  total  order,  not  necessarily  in  a  total  order.\n",
        " So  if  two  of  us  start  chatting  at  the  same  time,  it's  fine  that  one  of  you  hears  the  first  message  and  then  the  second  and  the  other  of  you  hears  the  second  message  and  then  the  first.\n",
        " The  two  messages  are  parallel,  so  the  ordering  is  not  relevant  for  us.  What  we  want  to  order  are  those  messages  that  are  connected  by  an  offense  before  relationship.\n",
        " And  those  are  exactly  the  messages  that  are  in  an  ordering  with  the  vector  clock  ordering.  So  need  to  preserve  the  ordering  only  between  messages  and  replies.\n",
        " So  messages  that  are  in  an  offense  before  relationship,  total  order  and  multicast,  that  is  the  multicast  that  we  started  using  Glam  for  clock,  is  too  strong  because  it  guarantees  the  offense  before  ordering,\n",
        " but  it's  a  total  order.  So  it  tries,  I'm  talking  about  this  one.  So  this  one,  respect  the  offense  before  relationship.\n",
        " But  it's  also  total.  So  it  tries  and  succeeds  in  ordering  messages,  even  if  they  are  not  parallel.  Sorry,\n",
        " even  if  they  are  parallel  in  the  offense  before  relationship.  The  two  messages  are  parallel  in  the  offense  before  relationship,  so  they  were  limited  at  the  same  time.  And  the  totally  ordered  multicast  broadcast,\n",
        " the  totally  ordered  broadcast,  tries  to  order  them  in  a  certain  order.  Tries  to  put  the  first,  and  then  the  second  or  vice  versa  for  all  of  us.  This  is  something  that  we  don't  need,\n",
        " actually.  The  two  messages  are  parallel  in  the  offense  before  relationship.  We  may  accept  that  one  of  us  here  are  the  first  and  then  the  second,\n",
        " and  the  other  of  us  here  are  the  second  and  then  the  first,  depending  possibly  on  the  speed  of  the  channel.  Okay.  In  order  to  have  this  lower,  this  less  strong  ordering,\n",
        " what  we  can  do  is  using  vector  clock.  Using  vector  clock,\n",
        " we  use  a  variation  of  vector  clock  in  which  we  increment,  in  which  two  things  happen.  First,  we  don't  count  internal  events.\n",
        " We  only  count  events  of  sending  and  receiving  messages.  First.  And  second,  we  increment  a  clock  clock  only  at  sending  not  at  receiving  So  when  we  send  when  process  one  sends  it  increments  its  own  clock  But  when  process  two  receives  it  merge  the  two  blocks,\n",
        " but  does  not  increment  its  own  position  Right.  This  is  a  variation  of  vector  clock  that  still  has  all  the  properties  of  vector  clocks  And  we  need  this  variation  Okay,\n",
        " how  do  we  use  those  vector  clock?  Imagine  that  We  receive  a  certain  process  process  one  receives  a  message  Now  before  delivering  this  message  to  the  application  it  checks  that  those  two  Conditions  are  valid  So  it  holds  a  message  until  the  previous  message  are  received  and  in  particular  the  previous  messages  are  received  if  at  the  position  of  the  receiver  at  the  time  stamp  of  the  receiver  is  equals  Position  of  the  clock\n",
        " plus  one  at  the  time  stamp  of  all  the  other  position  is  strictly  lower  Let  me  give  you  an  example  here.  Okay.  Now  imagine  that  PI  sends  this  message  This  message  is  received  by  TJ  Then  PJ  sends  another  message  Okay,\n",
        " now  this  message  is  received  by  PK  Before  PK  receives  these  other  message  All  right,  so  this  is  that  the  situation  of  messages  messages.\n",
        " Imagine  that  now  this  message  and  this  message  are  in  an  app  test  before  relationship.\n",
        " This  message  was  received  by  PJ  and  this  causes  the  sending  of  the  new  message.  So  potentially  there  is  a  causal  relationship  between  those  two  messages.\n",
        " This  could  be  a  message  and  this  could  be  a  reply  to  that  message.  Question,  reply.  You  made  me  a  question.  I  replied.  You  made  me  a  question.\n",
        " I  replied.  This  student  received  my  reply  before  receiving  the  question  of  his  colleague.  Now  this  is  bad.\n",
        " The  fact  is  that  you  have  to  put  yourself  in  the  position  of  this  student.  This  student  received  this  message.  How  can  this  student  understand  that  this  message  is  a  reply  to  something  that  it  has  not  received  yet?\n",
        " So  how  can  this  student  understand  that  it  must  hold  this  reply  waiting  for  this  message  to  arrive?  By  only  looking  at  its  own  vector  clock  and  the  vector  clock  of  the  message.\n",
        " Now  you  may  reason  this  way.  You  say,  my  vector  clock  is  0,  0,  0.  I'm  receiving  1,  1,  0.  So  it's  fine  that  for  the  position  of  the  sender,\n",
        " my  vector  clock  is  lower  exactly  by  1  of  the  position  of  the  sender.  This  is  fine  because  the  sender  has  sent  this  message.\n",
        " So  it  has  incremented  its  own  clock.  Hold  messages  and  feel  some  condition  is  okay.\n",
        " Okay.  Yeah  Yes,  if  you  miss  messages,  you  may  have  problems  actually  if  Yes  The  the  idea  is  that  you  must  There  is  this  communication  in  which  this  is  a  broadcast  communication  which  all  messages  are  sent  to  everyone  Okay  question  I  Took  these  definitions  From  the  book  and  I  didn't  change  it  but  the  Hypothesis  are  stronger  than  required  The  hypothesis  are  that  messages  and  replies  are  sent  using  reliable  Five  for  the\n",
        " channel.  Now,  which  of  these  hypothesis  is  too  strong?  So  the  fact  that  the  messages  are  sent  to  all  the  boards  in  parallel  We  just  commented  is  that  required  Hypothesis  now,\n",
        " which  of  the  other  hypothesis  is  too  strong.  It's  not  required  Why  In  particular,\n",
        " which  one  of  the  two  conditions  Okay,\n",
        " we  have  two  conditions  there  only  one  of  the  two  is  the  one  that  is  fundamental  Because  one  says  that  the  timestamp  of  the  message  at  each  position  Exclude  the  second  says  the  timestamp  of  the  message  at  each  position  Excluding  the  position  on  the  center  is  lower  or  equal  than  the  vector  clock  of  the  receiver  receiver.\n",
        " Does  this  order  messages  in  FICO  order?  FICO  means  sender  to  receiver.  So  this  second  condition  does  not  guarantee  FICO  ordering.\n",
        " It's  the  first  condition  that  guarantees  FICO  ordering  because  it  says  the  timestamp  of  the  sender,  the  timestamp  of  the  message  at  the  sender  position  must  equal  my  own  view  of  the  sender  exactly  plus  one.\n",
        " If  instead  of  saying  equals  vkj  plus  one,  there  was  greater  or  equal  than  vkj,  then  that  would  require  the  FICO.\n",
        " It's  exactly  this  condition  that  guarantees  FICO.  This  condition  is  saying  the  timestamp  of  the  message  that  I  receive  is  such  that  at  the  position  of  the  sender,\n",
        " it  is  my  view  plus  one.  This  means  that  if  this  channel  would  not  be  FICO  ordering,  only  two  processes,\n",
        " p0,  p1,  p0  sends  a  message  and  then  sends  a  message.  There  are  two  messages.\n",
        " So  here  we  have  1,  0  and  here  we  have  2,  0.  The  two  messages  are  enough  as  before  relationship  because  they  have  been  sent  by  the  same  process.\n",
        " The  channel  is  not  FICO.  Now,  I  receive  with  0,  0.  So  my  clock  is  0,  0.  I  receive  2,  0.\n",
        " Now,  at  the  position  of  the  sender,  my  view  is  lower,  which  per  se  is  fine.  My  view  is  lower.  He  knows  better  than  me  what  happens  at  him.\n",
        " I  could  accept  it  in  principle,  but  I  don't  accept  it  because  I  don't  simply  check  that  his  own  view  is  greater  than  mine.  I  check  that  his  own  view  is  exactly  my  view  plus  1,\n",
        " exactly  plus  1.  So,  I  would  not  accept  this.  I  would  wait  for  this.  Now,  I  accept  1  is  exactly  my  view  plus  1.\n",
        " So,  my  view  becomes  1  plus  2,  and  then  I  delay  this  one  here,  and  then  2.  So,  it's  the  first  condition  that  is  the  one  that  is  important.  And  the  way  is  the  way  in  which  it  is  made,\n",
        " exactly  saying  that  I  must  wait  until  it  is  his  position  plus  1.  I  accept  them  if  they  satisfy  this  condition.\n",
        " I  put  them  in  the  queue  if  they  don't  satisfy  my  condition.  And  each  time  I  change  my  view,  so  I  update  my  vector  clock,  I  recheck  all  the  messages  that  are  in  the  queue  and  see  if  some  of  them  can  be  delivered.\n",
        " When  I  update  my,  if  I  have  a  long  queue,  when  I  update  my  vector  clock,  I  take  all  the  messages  that  are  in  the  queue,  and  all  of  them  that  satisfy  that  condition  are  sent  to  the  application.\n",
        " You  can  send  all  of  them.  there's  no  ordering,  no  ordering  is  important.  So,  you  check  if  one  of  them  satisfied  the  condition,  imagine  that  two  of  them  satisfied  the  condition,\n",
        " fine,  we'll  deliver  both  of  them.  (man  speaking  off  mic)  (man  speaking  off  mic)  - Okay.\n",
        " (man  speaking  off  mic)  (man  speaking  off  mic)  - Let  me  redraw.  So,\n",
        " there  is  this,  so  let  me  use  P0,  P1,  and  P2,  okay,  that's  also  good.  P0,  P1,  and  P2,  then  there  is  this  message  that  is  delayed,\n",
        " okay,  not  so  much.  So,  this  message  is  delayed,  so  now  here,  there  is  a  message  that  is  delivered,\n",
        " and  another  message  that  is  delivered,  okay.  This  is  the  worst  possible  situation,  possible.  One  of  the  worst  possible,  okay.  So,  this  starts  with  one,\n",
        " zero,  zero.  Now,  this  is  one,  one,  two,  three.  Zero,  and  this  is  one,\n",
        " two,  zero,  okay.  I  receive  one,  two,  zero.  My  clock  is  zero,  zero,  zero.  I  receive  one,  two,  zero.  So,  two  is  not  fine  because  there  is  the  problem  with  plus  one,\n",
        " and  one  is  not  fine  because  I  have  zero,  and  this  one  should  be  at  least  one,  okay.  So,  I  take  this  and  I  put  it  into  the  queue,  one,  two,  zero.  then  I  receive  this  one  again  now  this  position  is  fine  but  the  first  position  is  not  fine  so  I  put  it  into  the  queue  also  one  one  zero  now  I  receive  this  message  one  zero  zero  it  satisfies  all  the  condition  because  of  the  other  two  we  are  equal  and  at  the  first  it\n",
        " is  a  myu  plus  one  so  I  update  my  clock  one  zero  zero  I  go  there  and  I  check  the  mess  now  only  one  of  them  so  I  deliver  this  so  I  deliver  this  and  my  clock  becomes  so  I'm  delivering  this  one  now  my  clock  becomes  one  one  zero  I  recheck  and  now  I  can  deliver  this  one  so  basically  the  fact  that  you  every  time  we  check  and  update  we  check  and  update  guarantees  that  the  ordering  is  called  is  the  protocol  this  even\n",
        " better  situation  would  be  we  had  the  fourth  process  and  we  had  two  messages  from  the  okay  so  this  is  even  bad  and  perhaps  this  is  sent  this  is  posted  by  and  this  is  received  immediately  then  there  is  another  process  of  this  is  speed  free  there's  another  process  here  it  received  this  message  and  it  also  sends  a  message  that  is  caused  by  few  messages  caused  by  and  this  is  still  to  be  received  so  zero  zero  zero  one\n",
        " zero  and  this  is  one  zero  one  zero  okay  so  this  I  receive  one  zero  one  zero  I  cannot  accept  it  I  receive  one  one  zero  zero  I  cannot  accept  it  so  now  I  have  in  the  queue  one  one  zero  zero  and  one  zero  one  zero  okay  and  all  of  them  are  both  of  them  are  in  the  queue  now  I  receive  this  one  both  of  them  have  been  delayed  because  of  the  first  position  the  second  those  position  are  fine  this  was  sent  by  the  second\n",
        " and  I  have  zero  and  it  has  one  and  this  would  be  fine  and  for  this  this  was  sent  by  P2  so  at  third  position  it  is  exactly  one  plus  me  then  this  is  fine  the  problem  is  with  the  first  position  both  of  them  okay  now  I  receive  this  message  my  cloth  becomes  one  zero  zero  okay  so  one  zero  zero  zero  okay  I  did  it  now  I  go  back  and  I  cannot  set  both  of  them  and  I  can  deliver  both  of  them  in  any  order  it's  not\n",
        " important  because  those  two  are  parallel  those  two  messages  in  the  happens  before  relationship  are  parallel  are  both  caused  by  this  message  but  they  are  parallel  with  each  other  so  I  can  deliver  in  any  order  this  guarantees  that  I  respect  their  relationship  if  they  are  parallel  I  can  deliver  in  any  order  and  in  fact  I  take  from  the  queue  in  any  position  they  are  and  then  look  okay  usual  exclusion  is  already  guaranteed\n",
        " by  trying  to  guarantee  this  can  be  the  first  of  the  two  conditions  so  even  if  the  channels  are  not  pipe  ordering  and  so  there  is  one  message  overcomes  the  second  in  the  channel,\n",
        " then  actually,  this  is  not  a  problem.  Because  that  condition  avoids  the  situation.  No,  it  doesn't  avoid  it.  It  corrects  this  situation.  Because  it  will  hold  the  message  until  the  pipe  order  is  going.\n",
        " [INAUDIBLE]  And  the  messages  are  presently  in  parallel.  In  broadcast?  Not  necessarily  in  parallel,  but  in  broadcasting.  And  the  channels  are  reliable.\n",
        " If  you  lose  messages,  you  have  a  mess.  Because  you  are  waiting  forever,  basically.  OK,  mutual  exclusion.  Mutual  exclusion  are  required  to  prevent  interference  between  processes.\n",
        " The  typical  example  of  mutual  exclusion  happens  in  a  multi -threaded  application  when  you  try  to  access  the  same  variable,  the  same  area  of  memory.  Two  threads  try  to  update  the  same  area  of  memory.\n",
        " They  crash.  They  go  in  conflict  with  each  other.  This  is  resolved  in  a  multi -threaded  application  by  leveraging  the  faculty  of  a  single  clock.\n",
        " In  a  multi--  in  a  distributed  system,  you  don't  have  a  single  clock.  So  you  have  to  invent  some  protocol  to  guarantee  mutual  exclusion.  Example  of  mutual  exclusion,  printing.  Several  processes  want  to  access  a  single  printer  and  want  to  print  a  document.\n",
        " And  you  don't  want  the  two  documents  to  be  printed  in  parallel.  You  want  all  the  pages  of  the  first  document  to  be  printed,  and  then  all  the  pages  of  the  second  process  to  be  printed.\n",
        " So  basically,  you  want  those  three  properties.  It's  safety,  aliveness,  and  an  optional  ordering  property.  The  safety  properties  says  that  at  most  one  process  exits  the  critical  section  at  the  top.\n",
        " The  critical  section,  in  our  example,  is  the  print.  processes  are  reliable,\n",
        " of  course,  in  place.  Now,  the  simplest  solution  is  the  solution  that  is  actually  adopted  by  a  printer.  Our  printer  works.\n",
        " Basically,  the  idea  is  that  there  is  a  single  process  that  controls  the  access  of  the  printer.  When  you  have  two  users  that  try  to  print  a  document,\n",
        " actually  the  applications  of  the  two  users  do  not  take  full  control  of  the  printer.  They  do  not  access  the  printer  directly.  They  access  the  printer  server.\n",
        " And  the  printer  server  access  the  printer.  So  there  is  a  single  process,  centralized  process,  that  controls  the  access  to  the  critical  section,  to  the  shared  resource.\n",
        " That  was  the  memory  in  the  example  of  multi -tread  application.  It  was  the  printer  in  the  example  of  the  printing,  the  shared  resource  in  general.  So  you  have  a  centralized  component  that  receives  the  request.\n",
        " It  has  a  single  clock,  so  it  may  order  and  guarantee  that  the  access  to  the  shared  resource  is  given  in  an  atomic  way  because  it  has  a  single  clock.\n",
        " So  the  simple  solution  is  having  a  single  process  that  receives  the  request,  or  having  a  single  process  that  manages  the  right  to  access  the  shared  resource.\n",
        " So  it's  not  the  printer  server  that  directly  prints,  but  the  printer  server  gives  each  process  one  by  one  the  right  to  print.\n",
        " So  you  ask  me,  may  I  print?  And  I  will  see  two  requests.  I  look  at  which  one  of  the  two  arrived  first.  And  then  I  say,  OK,  one  of  you,\n",
        " I  say  yes.  To  the  other  of  you,  I  say  no,  you  have  to  wait.  Now,  you  go  access  the  printer  directly,  print,  print  all  your  documents,  all  your  100  pages.\n",
        " Then  you  come  back,  you  give  me  back  the  token,  the  token  that  gives  you  the  right  to  print.  And  then  I  know  that  the  printer  is  free,  and  I  can  give  the  token  to  the  second  process,\n",
        " and  so  on  so  forth.  This  is  the  idea  of  having  a  centralized  process  that  It  is  not  printing,\n",
        " it  does  not  hold  the  resource,  so  it's  not  printing,  and  it  was  not  interested  in  printing,  so  it  did  not  send  a  message  requesting  to  access  the  printer  before,\n",
        " so  it  is  not  printing,  it  did  not  send  a  message  similar  to  the  one  that  it  received  where  it  requests  to  access  the  printer,  then  it  simply  replies  with  the  knowledge,\n",
        " so  I  would  like  to  print  and  it  says  okay  you  can  go,  okay  you  can  go,  most  of  you  would  say  me  you  can  go,  because  you  are  not  printing  and  you  are  not  interested  in  printing.\n",
        " Now  if  the  process  is  printing,  so  the  process  holds  the  request,  it  puts  the  request  in  a  local  queue  without  reply,\n",
        " and  this  local  queue  is  ordered  according  to  the  timestamp  of  the  request.  Finally,  and  this  is  the  third  and  most  important  point,  the  situation  in  which  I  say  to  all  of  you  I  would  like  to  print,\n",
        " and  one  of  you  receive  this  message  after  having  sent  a  similar  message,  so  I  say  I  would  like  to  print,  and  you  receive  the  message,  but  he  also  said  I  would  like  to  print,\n",
        " so  imagine  that  I  receive,  I  say  I  would  like  to  print,  and  before  having  been  acknowledged  by  all  of  you,  so  I  receive  a  three  acknowledged,  I  am  waiting  100  acknowledgment,\n",
        " I  receive  only  three  of  them,  and  in  the  meanwhile  I  receive  a  request  from  him  saying  I  also  would  like  to  print,  so  not  an  acknowledgement,  but  a  request,  okay.  Now  I  check  at  the  two  messages  my  old  request,\n",
        " and  the  request  that  I  received,  if  my  own  request  at  the  timestamp  that  is  lower  than  the  request  of  the  other,\n",
        " so  my  request  case  first,  because  it  has  a  lower  time  stamp,  it  has  a  lower  than  for  block,  then  I  hold  the  request  and  don't  acknowledge.\n",
        " If  it's  all  request  has  a  lower  time  stamp  than  my,  so  imagine  that  my  time  stamp  was  10,  and  I  receive  a  request  time  stamp  at  8,  I  say  okay  go  print,\n",
        " instead  I  receive  12,  my  request  was  10,  I  receive  12,  I  don't  reply,  and  I  hold  the  request  in  the  queue.  So  basically  each  process  resolve  the  ties,\n",
        " the  competition  locally  by  looking  at  the  lamp  or  clock  of  its  own  request  and  the  lamp  or  clock  of  the  other  request  that  it  received.  Now  you  easily  understand  that  only  one  of  the  parallel  requests  or  receives  all  the  acknowledgement.\n",
        " When  you  receive  all  the  acknowledgement,  then  you  know  that  you  can  print,  you  have  the  resource,  so  you  start  printing,  you  use  the  printer  as  long  as  you  want.\n",
        " When  you  finish  printing,  so  on  releasing  the  resource  the  process  TI  acknowledged  all  the  queue  direct  request.  So  while  I  was  printing,\n",
        " I  have  a  lot  of  requests  in  my  queue,  after  ending  printing,  those  requests  or  requests  that  I  received  then  did  not  acknowledge.  When  I  finish  printing,  I  acknowledge  all  that  in  any  order.\n",
        " Actually  the  fact  that  you  keep  the  request  in  order  is  not  correct.  And  you  may  easily  understand  how  the  system  works  and  the  fact  that  it's  easy  to  understand  that  it  guarantees  a  safety  property,\n",
        " so  at  most  one  process  execute  the  critical  section,  so  at  most  one  process  will  print.  Because  if  there  is  another  process  printing,\n",
        " then  that  process  would  not  acknowledge.  And  if  there  is  no  another  process,  but  no  two  processes  at  the  same  time  may  receive  the  acknowledgement  by  all  the  other.\n",
        " Because  in  order  to  receive  the  acknowledgement  by  all  the  other,  you  should  acknowledge  me,  me,  but  in  order  for  you  to  receive  that  knowledge  by  all  the  other,  you  should  have  been  acknowledged  by  me.  So  either  you  acknowledge  me  or,\n",
        " I  acknowledge  you,  depending  on  the  timestamp,  but  not  the  two  acknowledged  can  happen  in  parallel,  okay?  So,  only  one,  at  most  one  process  go  printing  at  each  time.\n",
        " The  liveness  property  is  also  satisfied.  Sooner  or  later,  you  will  get  acknowledged.  And  this  is  guaranteed  by  the  fact  that  there  is  always  a  total  order  between  the  numbers.\n",
        " And  the  same  fact  that  there  is  a  total  order  between  the  request  guarantees  the  optional  property.  Because  this  total  order  is  the  total  order  of  Lamport,  and  the  total  order  of  Lamport  is  stronger  than  the  happens  before  order,\n",
        " so  it's  longer  than  the  causal  order.  So,  you  will  guarantee  that  if  two  requests  happens  in  an  happens  before  relationship,  then  the  first  is  given  the  right  to  access  the  shared  results  before  the  second.\n",
        " Okay?  Questions?  Second  approach.  This  does  not  require  any  centralized  process.\n",
        " Second  approach,  token  ring.  Again,  we  are  all  together.  We  want  like  to  access  this  printer  without  printing  in  parallel.  So,\n",
        " we  organize  ourselves  in  a  ring.  You  can  say  I,  precede  you,  then  precede  you,  then  precede  you,  and  this  ring.  So,\n",
        " it's  a  logical  ring.  It's  not  a  physical  ring.  So,  we  organize  all  the  process  in  a  ring,  and  we  start  circulating  a  token  into  the  ring.  So  there  is  a  token  circulating  into  this  ring  continuously.\n",
        " At  each  time  you  receive  the  token  from  the  previous  process  in  the  ring,  the  token  is  just  a  message,  you  receive  the  token  from  the  previous  process  in  the  ring,\n",
        " if  you  want  to  print,  you  take  the  token,  if  you  don't  deliver  it,  you  access  the  printer,  if  you  don't  want  to  print,  you  pass  the  token.  So  the  token  continuously  goes  around  the  ring  until  there  is  someone  that  wants  to  print,\n",
        " takes  it,  print,  reprint,  reprint  the  token.  Since  there  is  a  single  token,  then  only  one  process  at  each  time  will  be  given  the  right  to  print,\n",
        " so  the  safety  property  is  guaranteed.  The  liveness  property  is  also  guaranteed,  because  sooner  or  later  you  will  receive  the  token,\n",
        " unless  there  is  a  process  that  takes  the  printer  forever,  but  clearly  this  is  a  situation  that  would  affect  any,  any  and  every  protocol.\n",
        " So  the  assumption  is  that  if  the  process  is  given  the  right  to  access  the  critical  section,  sooner  or  later  it  leaves  the  critical  section,  so  it  doesn't  crash  or  stops  forever  into  the  critical  section.\n",
        " So  the  liveness  property  is  satisfied.  So  now  I  have  a  question  for  you,  is  the  optional  fairness  condition  satisfied?\n",
        " So  is  the  access  to  the  printer  given  in  AppSv4  order?  No,  because  it  is  given  in  ring  order.  order.  So  it  could  be  that  process  number  zero  would  like  to  print.\n",
        " But  the  token  at  this  time  is  at  position  one.  So  it  must  wait  for  the  entire  ring  to  be  done.  Then  after  process  zero  decided  to  print,\n",
        " process  four  decided  also  to  print.  It  decided  to  print  just  before  receiving  the  token.  So  it  is  given  the  right  before  process  zero.  So  clearly  you  violate  the  appense  before  ordering,\n",
        " but  still  the  system  is  alive.  So  always  alive.  Very  simple  comparison.  Centralized  solution.\n",
        " I  ask  the  right  to  print  to  a  centralized  server  that  it  gives  me  the  right  or  refuse  me  the  right  until  someone  else  free  the  printer.\n",
        " Distributed  with  LEMPORT,  distributed  with  token  ring.  So  how  many  messages  are  required  in  the  best  possible  situation  in  order  to  print?\n",
        " Two  messages.  I  ask  the  centralized  server  if  it  replies  OK,  two  messages.  With  LEMPORT,  how  many  messages?  I  must  ask  to  all  of  you  n  minus  one  messages.\n",
        " And  all  of  you  must  acknowledge  n  minus  one  messages.  So  two  per  n  minus  one.  In  the  token  ring,  how  many  messages?\n",
        " It  depends.  I  could  be  very  lucky.  And  I  decided  to  print  just  before  receiving  the  token.  So  just  one  message.  Or  it  could  be  that  the  token  remaining  circulating  for  hours  and  hours  and  hours  before  someone  decides  to  print.\n",
        " So  there  is  an  overhead  that  can  be  potentially  infinite  in  order  to  access  the  print.  Delay  before  entry.  How  long  time  I  must  wait?\n",
        " Let  me  count  that  in  latency  time.  Never  reach  latency  time.  Two  average  latency  time  before  entry.  entry.  Now,\n",
        " for  Lamport,  do  you  want  to  create  if  no  one  wants  to  create?\n",
        " How  long  do  I  have  to  wait?  I  have  to  message  all  of  you  and  I  have  to  receive  an  acknowledgement  by  all  of  you.\n",
        " So,  I  would  say  two  and  minus  one,  but  I  would  also  say  two.  Potentially,  I  could  request  all  of  you  in  broadcast  if  there  is  a  broadcasting  layer.\n",
        " If  there  is  a  broadcasting  layer  at  the  network  layer,  this  only  requires  one  latency  to  be  paid  if  I  can  do  everything  in  parallel.  If  I  do  everything  sequentially,\n",
        " then  I  send  to  you  and  I  wait  for  you,  then  I  send,  then  I  send,  then  I  must  wait  two  and  minus  two  per  and  minus  one.  But  if  I  do  it  in  parallel,\n",
        " potentially  this  could  become  two.  And  for  the  token  ring  solution,  in  the  best  situation,  I  receive  the  token  and  I  immediately  decide  to  print.\n",
        " So,  potentially  even  zero  time  to  print  or  I  decide  to  print  just  after  passing  the  token.  So,  I  have  to  wait  for  an  entire  round  in  order  to  print  again.\n",
        " Potential  problems.  For  a  centralized  solution,  if  the  coordinator  crash,  we  have  a  problem.  But  for  the  lamp  work  solution,  if  any  process  crash,\n",
        " we  have  a  problem.  And  for  the  token  ring  solution,  if  any  process  crash,  we  have  a  problem.  And  to  the  color  cloth  to  be  the  same,\n",
        " OK?  Lateral  action.  Just  in  the  previous  topic,  I  mentioned  the  fact  that  we  could  have  the  need  of  a  single  process  that  coordinates  something,\n",
        " a  single  process  that  coordinates  the  access  to  the  printer,  OK?  Now,  this  single  process  becomes,  we  said,  a  single  point  of  failure,\n",
        " so  it  would  be  nice  to  have  the  possibility  of  electing  this  process.  So  we  all  agree  that  it  is  the  leader  for  the  printer.  Then  if  it  goes,\n",
        " then  we  elect  another  leader.  So  leader  election  is  important  in  a  lot  of  situations  in  which  you  need  to  elect  a  single  process  that  must  coordinate  the  action  of  several  other  processes,\n",
        " OK?  And  again,  for  leader  election,  we  have  several  protocols.  The  problem  is  that  we  have  to  agree  on  something.\n",
        " So  it's  an  agreement  problem.  And  we  know  how  complex  it  is  to  agree  on  something.  Some  assumptions.  The  minimal  assumptions  is  that  no  doubt  distinguishing,\n",
        " OK?  We  want  to  agree  that  someone  is  the  leader.  We  must  distinguish  one  process  from  the  other.  So  each  process  has  its  own  identifier  that  is  different  from  every  other  process,  very  simple  to  obtain.\n",
        " This  is  the  second  assumption.  We  assume  that  the  system  is  closed.  So  we  assume  that  all  processes  knows  the  group  within  with  the  agreement  must  happen.\n",
        " So  we  all  know  each  other.  The  group  is  closed.  And  for  simplicity,  in  most  of  my  example,  I  would  also  assume  that  there  are  no  crashes.\n",
        " So  the  group  remains  stable  for  the  duration  of  the  protocol.  If  there  are  crashes,  we  want  that--  now,  OK,  we  assume  that  no  processes  enter  the  group.\n",
        " Processes  may  crash  for  leader  election  action,  because  it's  exactly  the  situation  that  we  want  to  call.  But  we  assume  that  all  sooner  or  later  discover  that  the  leader  has  crashed.\n",
        " Or  at  least  one  of  us  is  able  to  discover  that  the  leader  has  crashed.  And  also  we  assume  crash  failures,  so  there  will  not  be  something  failures.\n",
        " So  omission  failures,  there  will  not  be  something  failures.  So  we  are  in  a  group,  we  elected  a  leader,  the  leader  crashes,  now  we  have  to  elect  a  new  leader.\n",
        " We  can  realize  we  know  each  other,  we  can  realize  that  the  leader  has  crashed,  and  we  assume  links  are  reliable.  First  protocol,\n",
        " bully  election.  The  bully  election  algorithm  is  very  simple.  We  assume  links  are  reliable,  and  we  assume  that  we  can  see  the  crash,\n",
        " which  is  a  strong  assumption.  Because  understanding  that  someone  crashed  means  that  we  are  in  a  synchronous  system.  That's  the  only  way  to  distinguish  between  a  very  long  channel  and  a  crashed  process.\n",
        " You  have  no  way  to  distinguish  it  unless  you  have  a  bound  on  the  transmission  time.  I  can  understand  the  cube  pressure  if  I  don't  know  that  there  is  a  maximum  time  for  me  to  communicate  with  you  and  vice  versa.\n",
        " It  could  be  that  you  don't  reply  me  simply  because  you  didn't  hear  me.  Because  the  master,  even  if  the  channel  is  reliable,  it  could  be  very,  very,  very,  very  slow.  okay?\n",
        " So  basically  we  are  assuming  to  be  in  a  synchronous  system.  The  Boolean  algorithm  is  very  simple.  When  a  process  P,  any  process  P,  notice  that  the  actual  coordinator  is  no  longer  responding,\n",
        " so  it's  no  longer  available,  so  it  is  forwarded,  it  crashes,  P  sends  an  elect  message  that  includes  its  own  ID  to  all  other  processes  with  an  ID  higher  than  in.\n",
        " Let  me  do  it  with  an  example.  Seven  was  the  previous  leader,  seven  crashed  it.  Four  discords  that  seven  crashed  it.\n",
        " So  four  sends  an  election  message,  this  election  message  has  number  four  inside,  and  the  message  is  sent  to  processes  whose  ID  is  greater  than  four,\n",
        " so  to  process  five,  six,  and  seven.  The  process  that  receive  an  elect  message  reply  with  a  HEO  message.\n",
        " So  with  a  message  that  says,  no,  you  cannot  be  the  leader.  So  if  I  receive  an  elect  message,  this  means  that  my  ID  is  greater  than  the  ID  of  the  sender.\n",
        " This  protocol  is  called  the  Boolean  protocol,  because  those  with  an  higher  ID  win,  the  Boole's  win,  okay?  So  five  says,  no,  you  cannot  be  the  leader.\n",
        " Six  says,  no,  you  cannot  be  the  leader.  And  just  after  saying  to  the  sender  that  he  cannot  be  the  leader,  the  processes  that  receive  an  elect  message  also  starts  another  election.\n",
        " So  five  sends  an  election  message  to  six  and  seven  with  its  own  number  inside.  And  again,  six  replies  to  five  saying,\n",
        " no,  you  cannot  be  the  leader.  It  also  made  an  election.  It  cannot  receive  a  KO  message  because  seven  will  not  reply.\n",
        " So  after  waiting  for  the  reply  from  seven  for  some  time,  time,  that  is  the  maximum  transmission  time,  six  elect  itself  as  the  leader  and  communicates  to  all  other  processes  that  is  the  new  leader.\n",
        " The  bully  wins.  The  process  with  the  highest  ID  that  remained  after  the  crash  wins.  These  words,  even  if  you  have  multiple  crash  in  parallel  and  even  if  you  have  multiple  processes  that  in  parallel  recognize  that  there  was,\n",
        " so  imagine  that  not  only  four,  but  also  two  at  the  same  time  of  four  discovers  that  seven  crashed.  So  four  and  two  starts  an  election  in  parallel.\n",
        " This  is  not  a  problem.  Two  sends  the  message  to  everyone,  including  four  and  three,  but  not  one  and  all  say  KO.\n",
        " You  cannot  be  a  leader.  As  soon  as  two  receives  a  message  that  says,  I  cannot  be  a  leader,  then  it's  aboard  its  own  election  and  wait  for  an  election  by  the  other.\n",
        " So  sooner  or  later,  even  if  there  are  multiple  crashes,  even  if  multiple  processes  at  the  same  time  discover  that  the  leader  crashed,  sooner  or  later  you  converge,\n",
        " the  protocol  converge  by  electing  a  single  process  as  the  new  leader.\n",
        " We  said  that  each  process  knows  the  idea  of  the  other  processes  and  so  four  knows  that  exist,  processes  from  zero  to  seven.\n",
        " And  it  broadcasts  to  all  of  them  that  has  a  90 -degree  there.  [INAUDIBLE]  Because  you  don't  know  potentially  that  two  crashed.\n",
        " Which  one  crashed?  OK,  so  the  election  basically  allows  everyone  to  discover  who  crashed.  Which  one  crashed  and  which  one  not?  Questions?\n",
        " OK,  this  is  the  bully  election.  The  other  approach  is,  again,  using  a  ring.  Ring  is  a  structure  as  you  discovered,\n",
        " probably  understood  during  the  course.  Ring  is  a  structure  that  is  very  commonly  used.  In  order  to  implement  proper  rules.\n",
        " Because  it  guarantees  a  certain  order  and  it  guarantees  certain  properties.  And  in  fact,  this  is  a  solution  that  we  may  also  use  to  elect  a  leader.\n",
        " We  assume  a  logical  ring  between  nodes.  When  a  process  detects  a  failure  of  the  leader,  it  sends  an  elect  message  containing  its  own  ID  to  the  closest  neighbor  that  is  still  available.\n",
        " OK,  so  here  in  this  example,\n",
        " at  the  same  time,  two  and  five  discovered  that  seven  crashed.  So  two  sends  an  election  message  to  three  with  its  own  ID.\n",
        " Three  circulates  the  message  adding  its  own  ID.  At  the  same  time,  five  discovered  that  seven  crashed.  And  so  sends  a  message  along  the  ring  with  its  own  ID.\n",
        " Six  received  this  message,  checks  who  is  the  next  in  the  ring,  and  sends  to  the  next  in  the  ring  adding  its  own  ID.\n",
        " And  the  system  proceeds  this  way  with  those  two  tokens  that  circulate  in  parallel.  parallel.  Once  the  process  receives  a  message  that  contains  its  own  ID,\n",
        " so  once  the  process  receives  a  message  with  its  own  ID,  so  imagine  at  a  certain  point  to  receive  this  token.  At  the  end  of  the  round  this  token  would  contain  the  IDs  of  all  the  processes  that  are  still  alive.\n",
        " So  when  a  process  receives  a  token  with  its  own  ID  inside,  this  means  that  the  entire  ring  has  been  followed  and  the  process  takes  the  list  of  IDs,\n",
        " takes  the  greatest  one  or  the  lowest  one  or  the  one  that  you  want.  The  only  important  thing  is  that  everyone  choose  the  same.  Let's  say  the  greatest  one  and  this  is  the  new  leader.\n",
        " And  this  information  must  be  circulated  for  another  round.  So  there  is  first  an  elect  phase  and  then  there  is  a  coordinate  phase,\n",
        " exactly  like  in  this  case  an  election  and  then  a  coordination  phase.  Full  ring,  when  the  message  arrives  back  to  me,\n",
        " I  choose  the  leader  and  I  circulate  a  coordinate  message  saying  who  is  the  leader.  And  if  you  have  two,  you  have  to  score  at  the  same  time,\n",
        " basically  you  have  two  tokens  that  goes  around,  both  the  two  that  start  that  of  both  two  and  five  sooner  or  later  will  receive  their  own  token.\n",
        " They  will  elect  the  same  leader  because  those  two  tokens  would  include  exactly  the  same  list  of  IDs,  IDs,  so  they  will  take  the  greatest  S6,  both  of  them  would  elect  six,\n",
        " and  so  we  would  have  two  tokens  going  around,  let's  say,  six  has  been  elected.  That's  fine.  Instead  of  one,  two,  but  it's  still  acceptable.\n",
        " At  the  end,  we  elect  six.  Okay?  Yeah?  What  happens  if  in  the  second  phase  we  elect  the  leader  and  practice?\n",
        " Basically  that  we  assume  that  he  is  the  leader,  because  the  coordinate  master  set  is  the  leader,  then  we  start  checking  if  the  leader  is  alive,\n",
        " we  discard  that  the  leader  is  not  alive,  and  then  we  start  another  election.  This  is  also  true  for  this,  even  in  this  case,  we  have  the  same  problem.  You  elect  someone  just  after  repression.\n",
        " Okay?  This  means  that  it  does  not  the  time  to  operate,  but  we  elect  the  king,  then  we  can  discard  the  king's  pressure,  then  we  start  an  election.\n",
        " Okay?  Questions?  Okay?  We  can  compare  the  two  algorithms,\n",
        " I'll  leave  this  to  you.  Global  state,  collecting  global  state,  and  then  termination  detection.  Capturing  global  state,  this  goes  back  to  the  problem  that  one  application  of  this  case  is  the  problem  that  we  already  encountered,\n",
        " the  problem  of  creating  a  snapshot  of  a  system.  There  are  several  situations  in  taking  a  snapshot  of  the  system,  one  is  probably  the  principal  situation,\n",
        " but  there  are  others.  There  are  several  situations  in  which  we  would  like  to  take  a  picture  of  a  system.  system,  because  we  want  to  restart  from  that  picture  if  there  is  a  crash,\n",
        " or  because  we  want  to  create  a  long  of  the  system  that  takes  multiple  pictures  of  the  system  and  says  the  various  places  in  which  the  system  was,\n",
        " because  you  want  to  check  if  a  certain  protocol  finishes,  or  because  you  want  to  check  if  a  certain  protocol  is  correct.  Imagine  that  your  correctness  formula  is  based  on  the  state  of  all  the  processes,\n",
        " which  is  what  is  called  one.  You  know  that  in  order  for  your  process  to  be  correct,  a  certain  formula  based  on  the  state  of  the  various  processes  must  be  valid.\n",
        " Imagine  the  example  of  the  bank,  or  this  example.  In  this  example,  we  have  multiple  nodes.  We  could  consider  those  nodes  as  multiple  different  banks.\n",
        " This  is  bank  A,  this  is  bank  B,  this  is  bank  C,  this  is  bank  D.  Each  bank  has  its  own  amount  of  money,  and  you  may  transfer  money  from  a  bank  to  the  other.\n",
        " When  a  transfer  is  received,  the  amount  of  money  of  the  sender  is  decreased  by  that  amount,\n",
        " and  the  amount  of  money  of  the  receiver  is  increased  by  that  amount.  Now,  in  this  system,  in  order  for  the  system  to  be  correct,  this  system  in  which  we  exchange  money,\n",
        " if  no  money  is  produced,  it's  only  exchanged,  in  order  that  the  typical  correctness  formula  says  that  at  each  time,  the  total  amount  of  money  must  be  constant.\n",
        " How  do  you  check  how  much  money  is  around?  Yes,  several  processes.  So  you  would  like  to  take  a  picture  of  the  system,  and  what  you  would  like  to  obtain  is  a  perfect  picture  of  the  system.\n",
        " So  a  picture  like  the  one  that  we  can  take  here,  a  picture  like  this  picture,  so  a  picture  in  which  someone  on  top  of  the  system  capable  of  observing  the  entire  system  in  zero  time,\n",
        " so  have  a  perfect  view  of  the  entire  system  in  zero  time,  may  take  this  picture.  But  this  God  does  not  exist.  Again.\n",
        " So  we  would  like  to  take  a  snapshot  of  the  system,  but  this  snapshot  of  the  system  cannot  be  taken.  So  what  we  can  do  is  approximate  it.\n",
        " And  here  we  go  back  to  what  I  said  about  multiple  snapshots.  Each  process,  what  you  would  like  to  have  is  a  perfect  snapshot.\n",
        " A  perfect  snapshot  is  a  vertical  line  here.  That's  the  picture  taken  by  that  God,  that  is  able  to  see  the  entire  system  with  zero  delay.  So  but  the  vertical  bar  is  a  picture  that  we  cannot  take.\n",
        " What  we  can  take  is  a  picture  of  P1  here,  and  the  picture  of  P2  here,  and  the  picture  of  P3  here.  So  each  process  is  pictured  at  different  times.\n",
        " Now  the  question  becomes,  any  of  those  different  times  is  valid  or  not?  We  would  say,  okay,  the  vertical  is  impossible,  but  if  we  take  not  a  perfect  vertical  line,\n",
        " but  a  line  in  which  the  delay  is  one  millisecond,  that's  enough.  No,  not  necessarily.  It  depends.  So  you  would  like  to  take  this  picture,\n",
        " but  you  cannot  say  that,  okay,  but  this  picture  is  more  or  less  enough  because  this  delay  is  one  millisecond  and  we  don't  care  about  one  millisecond.  Yes,  we  care.\n",
        " It  depends.  It  depends  on  the  messages.  Okay,  so  actually  what,  and  even  potentially,  if  nothing  happens,  imagine  that  you  are  in  a  situation  in  which  nothing  happens  for  two  seconds.\n",
        " So  even  this  cut,  this  is  called  the  cut.  Even  this  cut,  we  are  here,  we  have  two  seconds,  it's  fine.  Nothing  was  happening  in  those  two  seconds,\n",
        " so  that's  fine.  So  it  depends  on  the  messages.  If  one  millisecond  or  two  seconds  or  one  hour  is  acceptable  or  not.  Okay,  and  in  particular,\n",
        " we  can  define  this  concept  that  is  called  consistent  cut.  And  this  is  something  that  we  already  mentioned.  This  cut  is  consistent,  even  if  it  is  very  long  and  strange,\n",
        " it  is  consistent.  Let's  see  first  why  it  is  not  consistent.  This  is  not  consistent.  It  is  not  consistent  because  it  is  taking  a  picture  of  P1  here.\n",
        " So  before,  sorry,  a  picture  of  P2  here,  before  P2  sent  M2,  and  it's  taking  a  picture  of  P3  here  after  P3  received  M2.\n",
        " So  it  is  registering  the  fact  that  P3  received  message  too,  but  P2  did  not  send  message.\n",
        " And  this  is  the  only  thing  that  we  cannot  accept.  I  hear  something  from  you  and  you  never  said  that,  or  you  didn't  say  that  in  the  picture.\n",
        " In  the  picture,  I  have  a  really  nice  face,  laughing,  but  you  didn't  say  the  name.  name,  anything.\n",
        " Go  like  this,  like  this,  okay?  You  have  a  problem.  Okay,  how  do  we  solve  this?  So,  definition.  A  cat  of  a  system  for  both  of  N  processes,\n",
        " the  one  PN,  is  the  union  of  the  histories  of  all  processes  up  to  a  certain  event  that  is  potentially  different  for  each  process.\n",
        " So,  the  cat  is  the  history  of  process  one  up  to  event  K1,  union,  the  history  of  process  two  up  to  event  K2,\n",
        " union  blah  blah  blah.  Union  the  history  of  process  N  up  to  KN.  So,  the  cat  is  the  history  of  P1  up  to  this  event  plus  the  history  of  P2  from  the  very  beginning  up  to  this  event  plus  the  history  of  P3  up  to  this  event.\n",
        " What  the  history  is  the  sequence  of  events  that  happen  at  each  process.  This  is  a  cut.  So  the  history  is  a  sequence  of  events  that  happen  at  a  process  i  from  event  0  up  to  event  k _i.\n",
        " Taking  together  all  these  stories,  we  have  a  cut.  A  cut  is  consistent  if  for  every  two  events,  e  and  f,\n",
        " where  e  is  part  of  the  cut  and  f  happens  before  e,  this  is  the  happens  before  relationship.  So  e  is  part  of  the  cut,\n",
        " f  happens  before  e,  so  we  want  f  to  be  part  of  the  cut.  So  if  an  event  is  part  of  the  cut,  all  the  events  that  happens  before  it  must  be  part  of  the  cut  also.\n",
        " So  if  we  include  these  events  into  the  cut,  we  must  also  include  all  the  events  that  happens  before  this,  and  the  event  of  sending  happens  before  the  event  of  this  event.\n",
        " Atomically,  but  this  is  something  that  happens  on  a  single  process,  so  this  can  be  done  atomically.  Atomically  takes  an  episode  of  its  own  state  and  emits  a  special  token  on  all  the  outgoing  channels  that  it  has  with  the  other  processes.\n",
        " So  we  are  in  a  situation  in  which  processes  are  doing  something  like  the  example  of  the  banks  before,\n",
        " the  various  banks  that  communicate.  They  are  connected  by  channels,  not  necessarily  by  the  action  channels,\n",
        " so  we  will  see  each  channel  with  its  own  direction.  Some  of  them  will  be  by  the  action  or  not.  So  when  a  process  starts  the  snapshot,\n",
        " it  takes  its  own  state  on  this  and  emits  on  all  the  outgoing  channels  a  special  message  that  is  the  token,  the  marker,\n",
        " the  marker  of  the  protocol.  This  is  done  atomically.  Atomically  means  that  this  process  stops  doing  everything  else,\n",
        " saves  on  this,  emits  the  two  markers.  Then  it  restarts.  So  it's  very  short,  it's  a  very  short  operation,  but  this  operation  requires  to  be  executed  atomically,\n",
        " so  nothing  else  must  happen  in  between  the  operation  of  saving  and  the  operation  of  sending  the  token.  So  in  particular,  you  should  not  receive  other  messages  in  between  and  you  should  not  process  other  messages  in  between.\n",
        " When  a  process  receives  the  marker,  the  token,  I  send.  the  protocol  for  processes  from  this  point  up  to  this  point.\n",
        " Now,  what  I  didn't  set,  but  I  assume  it  as  clear,  is  that  in  the  meanwhile,  the  standard  operation  of  the  process  continues.\n",
        " So  if  we  see  the  map  here,  stop  the  application,  do  its  job,  then  continue  the  application.  During  the  saving  mode,  those  messages  not  only  are  saved  on  this  during  the  recording  mode,\n",
        " not  only  those  messages  are  recorded  on  this,  but  they  are  also  processed  according  to  the  application.  So  the  application  never  stops,  apart  from  those  very  short  time  required  to  perform  those  atomical  operations  that  are  local,\n",
        " so  are  very  fast.  If  you  see  the  token,  I  stop  recording.  Very  fast  operation.  I  receive  the  first  token,  I  emit  the  new  token,  I  store  on  this.  Very  fast  operations.  So  apart  from  those  very  brief  time  in  which  the  application  is  suspended  locally  in  order  to  perform  those  atomic  actions,\n",
        " the  operations  continue  during  the  long  time  that  is  required  for  all  the  protocol  to  complete.  At  the  end  of  the  protocol,  all  the  pieces  that  have  been  saved  on  the  values  this  represent  a  consistent  cut.\n",
        " Why  they  represent  a  consistent  cut?  So  the  theorem,  the  distributed  snapshot  algorithm  selects  a  consistent  cut.  Captures  a  consistent  cut.\n",
        " Why?  Why  this  is  true?  Well,  let  EI  and  EJA  be  two  events  that  occurs  the  first  at  process  high  and  the  second  at  process  J.\n",
        " And  those  two  events  are  such  that  EI  happens  before  EJA.  So  we  have  our  situation.  We  have  two  events,  one  happening  at  process  zero,\n",
        " one  happening  at  process  two.  And  suppose  that  E0  happens  before  E2.  two.  So  the  event  app  and  a  dot  process  zero  happens  before  the  another  event  that  app  and  a  dot  process.\n",
        " And  those  events  are  the  application  events.  Now  suppose  this  is  true,  and  let  me  demonstrate  this  theorem  by  supposing  that  it  doesn't  work.\n",
        " So  suppose  that  E  j  is  part  of  the  cut,  but  E  i  is  not  part  of  the  cut.  So  let's  imagine  that  the  distributed  natural  problem  do  not  work.\n",
        " So  it  captured  E  j  into  the  snapshot,  but  it  did  not  capture  the  I  into  the  snapshot.  So  E  j  was  saved,\n",
        " E  i  was  not  saved.  This  is  what  we  are  saying.  And  let's  see  if  this  is  possible.  If  E  j  was  saved,\n",
        " this  means  that  E  j  occurred  at  P  j.  E  j  is  an  event  that  occurred  at  P  j,  and  E  j  is  part  of  the  snapshot.  So  this  means  that  E  j  occurred  before  P  j  saved  its  state,\n",
        " right?  In  order  to  be  part  of  the  saving,  it  must  happen  before  the  saving.  If  it  happens  after  the  saving,  it's  gone.  So  it  must  happen  before  the  saving.  So  if  E  i  is  not  part  of  the  cut,\n",
        " then  this  means  that  E  i  occurred  after  P  i  saved  its  state.  Now  if  P  i  equals  P  j,  this  is  impossible.  So  assume  that  P  i  is  different  from  P  j.\n",
        " Suppose  M  1  and  M  h  is  the  sequence  of  messages  that  produce  this  relationship.  If  E  i  happens  before  E  j,\n",
        " then  E  i  must  be  an  event,  and  then  there  must  be  some  communication,  and  then  some  other  communication,  and  then  some  other  communication,  and  then  some  other  communication,  blah,  blah,  blah,  and  then  there  is  EJ  at  the  receiver  of  the  last  message.\n",
        " This  is  the  only  condition  by  which  EI  happens  before  EJ.  If  there  is  no  communication,  the  two  events  are  parallel,  okay?  So  M1  and  MH  is  the  sequence  of  messages  that  give  rise  to  the  relationship  EI  happens  before  EJ.\n",
        " If  EI  appears  after  EI,  we  say  this  state,  then  EI  is  sent  to  market  before  M1  and  MH.\n",
        " So  EI,  we  said  EI  is  not  part  of  the  cut.  So  EI  appeared  after  EI  saved  its  state.  So  EI  appeared  after  the  market  was  sent  because  when  it  saved  its  state,\n",
        " it  sent  the  market.  So  EI  E0  occurred  after  P0  saved  its  state.  So  there  is  first  the  sending  of  the  market,\n",
        " and  then  there  is  the  event  EI,  and  so  the  first  message.  So  if  EI  occurred  after  EI  saved  its  state,  then  EI  sent  the  market  in  front  of  M1  and  MH.\n",
        " M1  and  MH  is  the  sequence  of  messages  that  give  rise  to  this  relationship.  But  since  the  channels  are  ordered,\n",
        " and  since  when  I  receive  a  message,  I  either  process  the  message,  because  this  is  an  application  message,  or  if  it  is  a  marker,\n",
        " I  atomically  emit  the  other  markers.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "DOCUMENT = re.sub(r'\\n|\\r', ' ', DOCUMENT)\n",
        "DOCUMENT = re.sub(r' +', ' ', DOCUMENT)\n",
        "DOCUMENT = DOCUMENT.strip()"
      ],
      "metadata": {
        "id": "pq8J8QOQtVX5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\",device=0)\n"
      ],
      "metadata": {
        "id": "Gjzei7H101vt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "tokenizer = summarizer.tokenizer\n",
        "tokens = tokenizer.encode(DOCUMENT)\n",
        "\n",
        "print(len(tokens))\n",
        "\n",
        "\n",
        "# Calculate the number of chunks needed\n",
        "chunk_len = math.ceil(len(tokens) / 1024)\n",
        "chunksNumber = len(tokens)//chunk_len\n",
        "\n",
        "# Split the tokens into chunks\n",
        "chunks = [tokens[i:i+chunksNumber] for i in range(0, len(tokens), chunksNumber)]\n",
        "\n",
        "\n",
        "for chunk in chunks:\n",
        "\n",
        "    # Set max_length and min_length based on token count\n",
        "    max_length = len(chunk) // 2\n",
        "    min_length = len(chunk) // 5\n",
        "\n",
        "    chunkText = tokenizer.decode(chunk)\n",
        "\n",
        "\n",
        "    # Generate summary for each chunk without sampling (example)\n",
        "    summary = summarizer(chunkText, max_length=max_length, min_length=min_length, do_sample=True)\n",
        "\n",
        "    print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NnXQzbOE_e5",
        "outputId": "ae3fa78b-af0e-4677-dffc-626571faee94"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10998\n",
            "[{'summary_text': 'A vector clock defines a perfect isomorphism with respect to the happens before relationship. When an event happens at a process, it increments its own position. The event of sending messages is an event. So the vector clock of p1 becomes 2, 0, 0. Now p2 receives the message and the message is sent. The vector clock is put on top of this application message. At the end, this is the result of a clock that represents the vision of a process or regarding the other processes. So each time you send a message, you send it to the right place. You send the message to process two. At process two, the clock was 0,0,0. Now process two received something, so we increment the second position of its own vector clock. The first position of process one represents the view of how many events happen at one of the processes. And if you look at the clock of A and the algorithm, they are this one act as the result.'}]\n",
            "[{'summary_text': \"Using vector clock, we can order events according to causality, not exactly causality. In order to do so, we need a variation of vector clock. For example, imagine that we have a group communication system. We want to order questions and replies, messages in causal order, not in a total order. Using vector clock we can do this using a strong ordering, which we use in which we increment, in which two things happen in two places. We don't need to order messages that are connected by an offense before relationship, but those that are in an ordering with the vector clock ordering. The two messages are parallel, so the ordering is not relevant for us. So it tries and succeeds in ordering messages, even if they are not parallel. Tries to order broadcast, the totally ordered broadcast, tries to order them in a certain certain order, and then then the second or vice versa for all of us. We may accept that one of us here are the first and then the other of us are the second, depending on the speed of the channel.\"}]\n",
            "[{'summary_text': \"The Hypothesis are that messages and replies are sent using reliable Five for the channel. The idea is that you must There is this communication in which this is a broadcast communication which all messages are sent to everyone. This means that if this channel would not be FICO ordering, only two, p0 sends a message and then sends another message. There are two messages here we have 1, 0 and here we here we we have 2, 0. The two messages are enough as they have been sent by the same process. Now, at the position of the sender, my view is lower per se per se, which is fine. He knows better than me what happens at se. I could accept it in principle, but I don't accept it because I could. accept it simply because I don’t accept that his own view is greater than mine. So, which of these hypothesis is too strong? So the fact that the messages were sent to all the boards in parallel We just commented is that required Hypotheses are too strong.\"}]\n",
            "[{'summary_text': 'Man speaks off mic: \"There\\'s no ordering, no ordering is important\" Man: \"When I update my, if I have a long queue, I take all the messages that are in the queue, and all of them that satisfy that condition are sent to the application\" \"This is the worst possible situation, possible. This is the speed free process,\" he adds. \"This guarantees that I can deliver in any order this guarantees that their relationship I can take from the queue and in fact I take from any position and then look at the usual.\" \"There is no way to predict how long it will take to send a message,\" he says. \"I don\\'t know how much time it takes to send and receive a message. I don\\'t even know if I can predict the amount of time that it takes for a message to be sent and received\" \"It\\'s a miracle that we have a computer that works this way,\" says the man. \"It is a miracle we can do this.\"'}]\n",
            "[{'summary_text': \"Mutual exclusion is required to prevent interference between processes in a distributed system. The idea is that there is a single process that controls the access of the printer. The safety properties says that at most one process exits the critical section at the top. The critical section, in our example, is the print. processes are reliable, of course, in place. Now, you go access the printer directly, print, print all your documents, all your 100 pages. Then you come back, you give me back the token, the token that gives you the right to print. And then I know that the printer is free, and I can give the token to the second process, and so on so forth. This is the idea of having a centralized process that It is not printing, it does not hold the resource, so it's not printing. And it was not interested in printing, and it did not send a message requesting to access the printers before, so. it is notprinting. Now if the process holds the request, it puts the request in a local queue without reply, according to the timestamp of the request.\"}]\n",
            "[{'summary_text': \"Each process resolves the ties, the competition locally by looking at the lamp or clock of its own request and the lamp of the other request that it received. Only one, at most one process go printing at each time. Sooner or later, you will get acknowledged. And this is guaranteed by the fact that there is always a total order between the numbers. The liveness property is also guaranteed, because sooner or later you will receive the right to print, unless there is a crash that takes the printer forever. So now I have a question for you, the optional condition for you is the fairness is satisfied? So AppS4.4 could be given in order? No, because it could be in any order, so it could also be given order in order. So, the assumption is that if the process is given the right. to access the critical section, sooner or. later it doesn't crash or stops forever into the critical. section. This does not require any centralized process. Second approach, we are all together. We want like to access this printer without printing in parallel. We organize ourselves in a ring. You can say I, precedes you, then precede you, and this ring. It's not a physical ring.\"}]\n",
            "[{'summary_text': \"Lamport is an open source system that lets users control printers on the Internet. LEMPORT is a type of distributed network with a number of different types of printers. In the best possible situation, only two messages are required to print a token. But the overhead can be potentially infinite in order to access the print function. Lemport is based on the idea of a leader, a single process that coordinates the action of other processes, and a color cloth that can be used to represent the color of the color cloth. For a centralized solution, if the coordinator crash, we have a problem. But for the lamp work solution,  if any process crash, We have a Problem. And for the token ring solution, for the Token Ring solution, If any process crashes, We Have A Problem. We assume that the system is closed. So we assume that all processes knows the group within with the agreement must happen. So the duration of the protocol remains stable for each group. If there are crashes, we want that no processes enter the group. Processes entering the group may crash for election, because it's exactly the situation that we want to call it.\"}]\n",
            "[{'summary_text': \"We assume links are reliable, and we assume that we can see the crash, which is a strong assumption. Because understanding that someone crashed means that we are in a synchronous system. That's the only way to distinguish between a very long channel and a crashed process. We can realize we know each other, we can realize that the leader has crashed and we have to elect a new leader. First protocol, bully election. The other approach is, again, using a ring. Ring is a structure as you discovered, probably understood during the course. In order to implement proper rules, we may also use a logical ring. We assume a process detects a failure of the leader, it sends an elect message containing its own ID to the closest neighbor that is still available. At the same time, two and five discovered that seven crashed, so two sends an election message to three. Three circulates the message adding its ownID to three with its own message. The bully wins. The process with the highest ID that remained after the crash wins.\"}]\n",
            "[{'summary_text': \"There is first an elect phase and then there is a coordinate phase, exactly like in this case an election and then a coordination phase. Once the process receives a message that contains its own ID, the process takes the list of IDs, takes the greatest one or the lowest one, or the one that you want. The only important thing is that everyone choose the same. The system proceeds this way with those two tokens that circulate in parallel. parallel. It would like to take a picture of the system, and what you would like is a picture like the one we can obtain here. So what we can do here is to observe the entire system in zero time, so have a perfect view of the whole system. But God does not exist, so this picture may not exist. We can compare the two algorithms, I'll leave this to you. Global state, collecting global state, and then termination detection. Capturing global state is a problem that one application of this case is the problem that we already encountered, the problem of creating a snapshot of a system.\"}]\n",
            "[{'summary_text': \"Each process is pictured at different times. A cut is consistent if for every two events, e and f, where e is part of the cut and f happens before e, this is the happens before relationship. Atomically, but this is something that happens on a single process, so this can be done atomically. So when a process starts the snapshot, it takes its own state and emits a special message on all outgoing channels that it has done something. So we are in a situation in which processes are doing something like the banks before the banks. They are connected by channels that communicate by the various action channels, so we will see each channel with its own direction. Some of them will be by action or not, so the marker of the marker, the marker on the outgoing channels, is the marker for the action channel. This is the protocol of the protocol, the token of the token, the markers of the markers, the tokens of the tokens. It's called the Message Queuing Protocol (MQP)\"}]\n",
            "[{'summary_text': \"Theory: The distributed snapshot algorithm selects a consistent cut. EI and EJA are two events that occur the first at process high and the second at process J. And those two events are such that EI happens before EJA. If EI appears after EI, we say this state, then EI is sent to market before M1 and MH. So EI appeared after the market sent its state, so E0 occurred after P0 saved its state. So there is first the sending of the market, and then there is the event EI. So if E i is not part of the cut, then this means that E i occurred after E i. E j is an event that occurred at P j, and E J is part of. the snapshot. So this means E j occurred before P j saved its. state, right? In order to be part. of the saving, it must happen before the saving. If it happens after the saving,. it's gone. If there is no communication, the two. events are parallel, okay? So EJ at the receiver of the last message. This is the only condition by which EI happening before EJ.\"}]\n",
            "[{'summary_text': ''}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "chunkLen = math.ceil(11755/1024)\n",
        "\n",
        "\n",
        "\n",
        "chunks = [str[i:i+chunkLen] for i in range(0, len(str), chunkLen)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OgYmBGAz0ML",
        "outputId": "c831770a-f7de-4fd3-ec16-694d22514ae9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc8hsC6e1HLH",
        "outputId": "8101d993-af1a-4782-8f68-2199929096e1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/openai/whisper.git\n",
        "! pip install yt-dlp\n",
        "\n",
        "import sys\n",
        "import warnings\n",
        "import whisper\n",
        "from pathlib import Path\n",
        "import yt_dlp\n",
        "import subprocess\n",
        "import torch\n",
        "import shutil\n",
        "import numpy as np\n",
        "from IPython.display import display, Markdown, YouTubeVideo\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrR1_IBS-PYs",
        "outputId": "4a2e0514-6ff1-483c-df7a-8403df7f5c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-37b_fvk3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-37b_fvk3\n",
            "  Resolved https://github.com/openai/whisper.git to commit b38a1f20f4b23f3f3099af2c3e0ca95627276ddf\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (2.0.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (0.56.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (10.1.0)\n",
            "Collecting tiktoken==0.3.3 (from openai-whisper==20230918)\n",
            "  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230918) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230918) (2.31.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230918) (3.27.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230918) (3.12.4)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230918) (17.0.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230918) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230918) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20230918) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20230918) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230918-py3-none-any.whl size=798405 sha256=c99b4370521887be1ff1da77fc9d2fee23a00e283d7d03c56dd6675a9fc515bb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wkv9w7zj/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20230918 tiktoken-0.3.3\n",
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2023.10.13-py2.py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mutagen (from yt-dlp)\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex (from yt-dlp)\n",
            "  Downloading pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets (from yt-dlp)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2023.7.22)\n",
            "Collecting brotli (from yt-dlp)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: brotli, websockets, pycryptodomex, mutagen, yt-dlp\n",
            "Successfully installed brotli-1.1.0 mutagen-1.47.0 pycryptodomex-3.19.0 websockets-11.0.3 yt-dlp-2023.10.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Model selection** 🧠\n",
        "\n",
        "#@markdown As of the first public release, there are 4 pre-trained options to play with:\n",
        "\n",
        "#@markdown |  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
        "#@markdown |:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
        "#@markdown |  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\n",
        "#@markdown |  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\n",
        "#@markdown | small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\n",
        "#@markdown | medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n",
        "#@markdown | large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |\n",
        "\n",
        "#@markdown ---\n",
        "Model = 'base.en' #@param ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large']\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the model.**\n",
        "\n",
        "whisper_model = whisper.load_model(Model)\n",
        "\n",
        "if Model in whisper.available_models():\n",
        "    display(Markdown(\n",
        "        f\"**{Model} model is selected.**\"\n",
        "    ))\n",
        "else:\n",
        "    display(Markdown(\n",
        "        f\"**{Model} model is no longer available.**<br /> Please select one of the following:<br /> - {'<br /> - '.join(whisper.available_models())}\"\n",
        "    ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "BSfDK-q5-Ybi",
        "outputId": "1f096fd1-c127-4a0b-92f3-3d90e431fbaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:01<00:00, 100MiB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**base.en model is selected.**"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Video selection** 📺\n",
        "\n",
        "#@markdown Enter the URL of the Youtube video you want to transcribe, wether you want to save the audio file in your Google Drive, and run the cell.\n",
        "\n",
        "Type = \"Youtube video or playlist\" #@param ['Youtube video or playlist', 'Google Drive']\n",
        "#@markdown ---\n",
        "#@markdown #### **Youtube video or playlist**\n",
        "URL = \"https://youtu.be/L_Guz73e6fw\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the video.**\n",
        "\n",
        "video_path_local_list = []\n",
        "\n",
        "\n",
        "ydl_opts = {\n",
        "    'format': 'm4a/bestaudio/best',\n",
        "    'outtmpl': '%(id)s.%(ext)s',\n",
        "    # ℹ️ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n",
        "    'postprocessors': [{  # Extract audio using ffmpeg\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'wav',\n",
        "    }]\n",
        "}\n",
        "\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    error_code = ydl.download([URL])\n",
        "    list_video_info = [ydl.extract_info(URL, download=False)]\n",
        "\n",
        "for video_info in list_video_info:\n",
        "    video_path_local_list.append(Path(f\"{video_info['id']}.wav\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4co29Oz9_AXJ",
        "outputId": "ae7df905-0179-4543-b691-c0ec7f25e54f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://youtu.be/L_Guz73e6fw\n",
            "[youtube] L_Guz73e6fw: Downloading webpage\n",
            "[youtube] L_Guz73e6fw: Downloading ios player API JSON\n",
            "[youtube] L_Guz73e6fw: Downloading android player API JSON\n",
            "[youtube] L_Guz73e6fw: Downloading m3u8 information\n",
            "[info] L_Guz73e6fw: Downloading 1 format(s): 140\n",
            "[download] Destination: L_Guz73e6fw.m4a\n",
            "[download] 100% of  133.30MiB in 00:00:03 at 41.10MiB/s  \n",
            "[FixupM4a] Correcting container of \"L_Guz73e6fw.m4a\"\n",
            "[ExtractAudio] Destination: L_Guz73e6fw.wav\n",
            "Deleting original file L_Guz73e6fw.m4a (pass -k to keep)\n",
            "[youtube] Extracting URL: https://youtu.be/L_Guz73e6fw\n",
            "[youtube] L_Guz73e6fw: Downloading webpage\n",
            "[youtube] L_Guz73e6fw: Downloading ios player API JSON\n",
            "[youtube] L_Guz73e6fw: Downloading android player API JSON\n",
            "[youtube] L_Guz73e6fw: Downloading m3u8 information\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Run the model** 🚀\n",
        "\n",
        "#@markdown Run this cell to execute the transcription of the video. This can take a while and very based on the length of the video and the number of parameters of the model selected above.\n",
        "\n",
        "#@markdown ## **Parameters** ⚙️\n",
        "\n",
        "#@markdown ### **Behavior control**\n",
        "#@markdown ---\n",
        "language = \"English\" #@param ['Auto detection', 'Afrikaans', 'Albanian', 'Amharic', 'Arabic', 'Armenian', 'Assamese', 'Azerbaijani', 'Bashkir', 'Basque', 'Belarusian', 'Bengali', 'Bosnian', 'Breton', 'Bulgarian', 'Burmese', 'Castilian', 'Catalan', 'Chinese', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Estonian', 'Faroese', 'Finnish', 'Flemish', 'French', 'Galician', 'Georgian', 'German', 'Greek', 'Gujarati', 'Haitian', 'Haitian Creole', 'Hausa', 'Hawaiian', 'Hebrew', 'Hindi', 'Hungarian', 'Icelandic', 'Indonesian', 'Italian', 'Japanese', 'Javanese', 'Kannada', 'Kazakh', 'Khmer', 'Korean', 'Lao', 'Latin', 'Latvian', 'Letzeburgesch', 'Lingala', 'Lithuanian', 'Luxembourgish', 'Macedonian', 'Malagasy', 'Malay', 'Malayalam', 'Maltese', 'Maori', 'Marathi', 'Moldavian', 'Moldovan', 'Mongolian', 'Myanmar', 'Nepali', 'Norwegian', 'Nynorsk', 'Occitan', 'Panjabi', 'Pashto', 'Persian', 'Polish', 'Portuguese', 'Punjabi', 'Pushto', 'Romanian', 'Russian', 'Sanskrit', 'Serbian', 'Shona', 'Sindhi', 'Sinhala', 'Sinhalese', 'Slovak', 'Slovenian', 'Somali', 'Spanish', 'Sundanese', 'Swahili', 'Swedish', 'Tagalog', 'Tajik', 'Tamil', 'Tatar', 'Telugu', 'Thai', 'Tibetan', 'Turkish', 'Turkmen', 'Ukrainian', 'Urdu', 'Uzbek', 'Valencian', 'Vietnamese', 'Welsh', 'Yiddish', 'Yoruba']\n",
        "#@markdown > Language spoken in the audio, use `Auto detection` to let Whisper detect the language.\n",
        "#@markdown ---\n",
        "verbose = 'Live transcription' #@param ['Live transcription', 'Progress bar', 'None']\n",
        "#@markdown > Whether to print out the progress and debug messages.\n",
        "#@markdown ---\n",
        "output_format = 'all' #@param ['txt', 'vtt', 'srt', 'tsv', 'json', 'all']\n",
        "#@markdown > Type of file to generate to record the transcription.\n",
        "#@markdown ---\n",
        "task = 'transcribe' #@param ['transcribe', 'translate']\n",
        "#@markdown > Whether to perform X->X speech recognition (`transcribe`) or X->English translation (`translate`).\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown <br/>\n",
        "\n",
        "#@markdown ### **Optional: Fine tunning**\n",
        "#@markdown ---\n",
        "temperature = 0.15 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown > Temperature to use for sampling.\n",
        "#@markdown ---\n",
        "temperature_increment_on_fallback = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown > Temperature to increase when falling back when the decoding fails to meet either of the thresholds below.\n",
        "#@markdown ---\n",
        "best_of = 5 #@param {type:\"integer\"}\n",
        "#@markdown > Number of candidates when sampling with non-zero temperature.\n",
        "#@markdown ---\n",
        "beam_size = 8 #@param {type:\"integer\"}\n",
        "#@markdown > Number of beams in beam search, only applicable when temperature is zero.\n",
        "#@markdown ---\n",
        "patience = 1.0 #@param {type:\"number\"}\n",
        "#@markdown > Optional patience value to use in beam decoding, as in [*Beam Decoding with Controlled Patience*](https://arxiv.org/abs/2204.05424), the default (1.0) is equivalent to conventional beam search.\n",
        "#@markdown ---\n",
        "length_penalty = -0.05 #@param {type:\"slider\", min:-0.05, max:1, step:0.05}\n",
        "#@markdown > Optional token length penalty coefficient (alpha) as in [*Google's Neural Machine Translation System*](https://arxiv.org/abs/1609.08144), set to negative value to uses simple length normalization.\n",
        "#@markdown ---\n",
        "suppress_tokens = \"-1\" #@param {type:\"string\"}\n",
        "#@markdown > Comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations.\n",
        "#@markdown ---\n",
        "initial_prompt = \"\" #@param {type:\"string\"}\n",
        "#@markdown > Optional text to provide as a prompt for the first window.\n",
        "#@markdown ---\n",
        "condition_on_previous_text = True #@param {type:\"boolean\"}\n",
        "#@markdown > if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop.\n",
        "#@markdown ---\n",
        "fp16 = True #@param {type:\"boolean\"}\n",
        "#@markdown > whether to perform inference in fp16.\n",
        "#@markdown ---\n",
        "compression_ratio_threshold = 2.4 #@param {type:\"number\"}\n",
        "#@markdown > If the gzip compression ratio is higher than this value, treat the decoding as failed.\n",
        "#@markdown ---\n",
        "logprob_threshold = -1.0 #@param {type:\"number\"}\n",
        "#@markdown > If the average log probability is lower than this value, treat the decoding as failed.\n",
        "#@markdown ---\n",
        "no_speech_threshold = 0.6 #@param {type:\"slider\", min:-0.0, max:1, step:0.05}\n",
        "#@markdown > If the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence.\n",
        "#@markdown ---\n",
        "\n",
        "verbose_lut = {\n",
        "    'Live transcription': True,\n",
        "    'Progress bar': False,\n",
        "    'None': None\n",
        "}\n",
        "\n",
        "args = dict(\n",
        "    language = (None if language == \"Auto detection\" else language),\n",
        "    verbose = verbose_lut[verbose],\n",
        "    task = task,\n",
        "    temperature = temperature,\n",
        "    temperature_increment_on_fallback = temperature_increment_on_fallback,\n",
        "    best_of = best_of,\n",
        "    beam_size = beam_size,\n",
        "    patience=patience,\n",
        "    length_penalty=(length_penalty if length_penalty>=0.0 else None),\n",
        "    suppress_tokens=suppress_tokens,\n",
        "    initial_prompt=(None if not initial_prompt else initial_prompt),\n",
        "    condition_on_previous_text=condition_on_previous_text,\n",
        "    fp16=fp16,\n",
        "    compression_ratio_threshold=compression_ratio_threshold,\n",
        "    logprob_threshold=logprob_threshold,\n",
        "    no_speech_threshold=no_speech_threshold\n",
        ")\n",
        "\n",
        "temperature = args.pop(\"temperature\")\n",
        "temperature_increment_on_fallback = args.pop(\"temperature_increment_on_fallback\")\n",
        "if temperature_increment_on_fallback is not None:\n",
        "    temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temperature_increment_on_fallback))\n",
        "else:\n",
        "    temperature = [temperature]\n",
        "\n",
        "if Model.endswith(\".en\") and args[\"language\"] not in {\"en\", \"English\"}:\n",
        "    warnings.warn(f\"{Model} is an English-only model but receipted '{args['language']}'; using English instead.\")\n",
        "    args[\"language\"] = \"en\"\n",
        "\n",
        "for video_path_local in video_path_local_list:\n",
        "    display(Markdown(f\"### {video_path_local}\"))\n",
        "\n",
        "    video_transcription = whisper.transcribe(\n",
        "        whisper_model,\n",
        "        str(video_path_local),\n",
        "        temperature=temperature,\n",
        "        **args,\n",
        "    )\n",
        "\n",
        "    # Save output\n",
        "    whisper.utils.get_writer(\n",
        "        output_format=output_format,\n",
        "        output_dir=video_path_local.parent\n",
        "    )(\n",
        "        video_transcription,\n",
        "        str(video_path_local.stem),\n",
        "        options=dict(\n",
        "            highlight_words=False,\n",
        "            max_line_count=None,\n",
        "            max_line_width=None,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    def exportTranscriptFile(ext: str):\n",
        "        local_path = video_path_local.parent / video_path_local.with_suffix(ext)\n",
        "        export_path = drive_whisper_path / video_path_local.with_suffix(ext)\n",
        "        shutil.copy(\n",
        "            local_path,\n",
        "            export_path\n",
        "        )\n",
        "        display(Markdown(f\"**Transcript file created: {export_path}**\"))\n",
        "\n",
        "    if output_format==\"all\":\n",
        "        for ext in ('.txt', '.vtt', '.srt', '.tsv', '.json'):\n",
        "            exportTranscriptFile(ext)\n",
        "    else:\n",
        "        exportTranscriptFile(\".\" + output_format)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iOTFm1vPAVDh",
        "outputId": "e09d9c2c-0837-40e6-f8b4-08e742e226c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### L_Guz73e6fw.wav"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:03.840]  We have been a misunderstood and badly mocked org for a long time.\n",
            "[00:03.840 --> 00:09.600]  Like when we started, we announced the org at the end of 2015,\n",
            "[00:10.800 --> 00:12.320]  and said we were going to work on AGI.\n",
            "[00:12.320 --> 00:14.320]  Like people thought we were batshit insane.\n",
            "[00:16.720 --> 00:23.760]  I remember at the time, a eminent AI scientist at a large industrial AI lab\n",
            "[00:24.320 --> 00:29.600]  was like DMing individual reporters being like, these people aren't very good,\n",
            "[00:29.600 --> 00:33.040]  and it's ridiculous to talk about AGI, and I can't believe you're giving them time of day,\n",
            "[00:33.040 --> 00:38.160]  and it's like that was the level of like pettiness and rancor in the field that a new group of people\n",
            "[00:38.160 --> 00:43.600]  say we're going to try to build AGI. So open AI and DeepMind was a small collection of folks\n",
            "[00:43.600 --> 00:52.160]  who were brave enough to talk about AGI in the face of mockery. We don't get mocked as much now.\n",
            "[00:53.040 --> 00:54.400]  Don't get mocked as much now.\n",
            "[00:56.720 --> 01:03.920]  The following is a conversation with Sam Altman, CEO of OpenAI, the company behind GPT-4,\n",
            "[01:03.920 --> 01:10.960]  JAD GPT, Dolly, Codex, and many other AI technologies, which both individually and together\n",
            "[01:10.960 --> 01:15.520]  constitute some of the greatest breakthroughs in the history of artificial intelligence,\n",
            "[01:15.520 --> 01:21.760]  computing, and humanity in general. Please allow me to say a few words about the possibilities\n",
            "[01:22.160 --> 01:26.720]  and the dangers of AI in this current moment in the history of human civilization.\n",
            "[01:27.360 --> 01:33.280]  I believe it is a critical moment. We stand on the precipice of fundamental societal transformation,\n",
            "[01:33.280 --> 01:38.640]  where soon nobody knows when, but many including me believe it's within our lifetime.\n",
            "[01:39.280 --> 01:45.680]  The collective intelligence of the human species begins to pale in comparison by many orders of\n",
            "[01:45.760 --> 01:53.840]  magnitude to the general superintelligence in the AI systems we build and deploy at scale.\n",
            "[01:55.200 --> 02:02.320]  This is both exciting and terrifying. It is exciting because of the innumerable applications\n",
            "[02:02.320 --> 02:09.360]  we know and don't yet know that will empower humans to create, to flourish, to escape the\n",
            "[02:09.360 --> 02:16.160]  widespread poverty and suffering that exists in the world today and to succeed in that old,\n",
            "[02:16.160 --> 02:23.440]  all-too-human pursuit of happiness. It is terrifying because of the power that super-intelligent\n",
            "[02:23.440 --> 02:30.960]  AGI wields to destroy human civilization, intentionally or unintentionally. The power\n",
            "[02:30.960 --> 02:39.040]  to suffocate the human spirit in the totalitarian way of George Orwell's 1984 or the pleasure-fueled\n",
            "[02:39.040 --> 02:46.240]  mass hysteria of Brave New World, where as Huxley saw it, people come to love their oppression,\n",
            "[02:46.800 --> 02:55.200]  to adore the technologies that undo their capacities to think. That is why these conversations\n",
            "[02:55.200 --> 03:01.200]  with the leaders, engineers and philosophers, both optimists and cynics, is important now.\n",
            "[03:02.800 --> 03:08.240]  These are not merely technical conversations about AI. These are conversations about power,\n",
            "[03:08.240 --> 03:13.280]  about companies, institutions and political systems that deploy, check and balance this power,\n",
            "[03:14.000 --> 03:20.480]  about distributed economic systems that incentivize the safety and human alignment of this power,\n",
            "[03:21.200 --> 03:27.520]  about the psychology of the engineers and leaders that deploy AGI, and about the history of human\n",
            "[03:27.520 --> 03:37.360]  nature, our capacity for good and evil at scale. I am deeply honored to have gotten to know and\n",
            "[03:37.360 --> 03:43.040]  to have spoken with on and off the mic with many folks who now work at OpenAI, including\n",
            "[03:43.040 --> 03:52.320]  Sam Altman, Greg Brockman, Iliya Sitzgever, Wojczyk, Saramba, Andrei Karpathy, Jakob Pachaki and many\n",
            "[03:52.320 --> 03:58.560]  others. It means the world that Sam has been totally open with me, willing to have multiple\n",
            "[03:58.560 --> 04:04.400]  conversations, including challenging ones, on and off the mic. I will continue to have these\n",
            "[04:04.480 --> 04:10.720]  conversations to both celebrate the incredible accomplishments of the AI community and the\n",
            "[04:10.720 --> 04:16.000]  steel man, the critical perspective on major decisions various companies and leaders make,\n",
            "[04:16.640 --> 04:24.400]  always with the goal of trying to help in my small way. If I fail, I will work hard to improve.\n",
            "[04:25.120 --> 04:31.280]  I love you all. This is the Lex Friedman podcast. To support it, please check out our sponsors in\n",
            "[04:31.280 --> 04:35.520]  the description. And now, dear friends, here's Sam Altman.\n",
            "[04:36.880 --> 04:42.480]  Hi Level. What is GPT for? How does it work and what to use most amazing about it?\n",
            "[04:43.120 --> 04:50.320]  It's a system that we'll look back at and say it was a very early AI, and it's slow, it's buggy,\n",
            "[04:50.960 --> 04:55.440]  it doesn't do a lot of things very well, but neither did the very earliest computers,\n",
            "[04:56.080 --> 05:01.840]  and they still pointed a path to something that was going to be really important in our lives,\n",
            "[05:01.840 --> 05:06.960]  even though it took a few decades to evolve. Do you think this is a pivotal moment? Out of\n",
            "[05:06.960 --> 05:13.200]  all the versions of GPT 50 years from now, when they look back at an early system that was really\n",
            "[05:13.200 --> 05:19.200]  kind of a leap in a Wikipedia page about the history of artificial intelligence, which of the\n",
            "[05:19.200 --> 05:24.080]  GPTs would they put? That is a good question. I sort of think of progress as this continual\n",
            "[05:24.160 --> 05:30.160]  exponential. It's not like we could say here was the moment where AI went from not happening to\n",
            "[05:30.160 --> 05:35.920]  happening, and I'd have a very hard time pinpointing a single thing. I think it's this very continual\n",
            "[05:35.920 --> 05:40.400]  curve. Well, the history books write about GPT one or two or three or four or seven.\n",
            "[05:41.280 --> 05:46.560]  That's for them to decide. I don't really know. I think if I had to pick some moment\n",
            "[05:47.200 --> 05:52.880]  from what we've seen so far, I'd sort of pick chat GPT. It wasn't the underlying model that\n",
            "[05:52.880 --> 05:56.560]  mattered. It was the usability of it, both the RLHF and the interface to it.\n",
            "[05:57.600 --> 06:03.520]  What is chat GPT? What is RLHF reinforcement learning with human feedback? What was that\n",
            "[06:03.520 --> 06:09.680]  little magic ingredient to the dish that made it so much more delicious?\n",
            "[06:10.560 --> 06:17.200]  So we train these models on a lot of text data, and in that process, they learn the underlying\n",
            "[06:18.080 --> 06:24.080]  something about the underlying representations of what's in here or in there. They can do\n",
            "[06:25.040 --> 06:29.840]  amazing things. But when you first play with that base model that we call it after you finish\n",
            "[06:29.840 --> 06:35.840]  training, it can do very well on evals. It can pass tests. It can do a lot of knowledge in there.\n",
            "[06:36.480 --> 06:43.840]  But it's not very useful, or at least it's not easy to use, let's say. And RLHF is how we take\n",
            "[06:43.840 --> 06:49.600]  some human feedback. The simplest version of this is show two outputs, ask which one is better\n",
            "[06:49.600 --> 06:54.720]  than the other, which one the human raiders prefer, and then feed that back into the model\n",
            "[06:54.720 --> 07:00.320]  with reinforcement learning. And that process works remarkably well, with in my opinion,\n",
            "[07:00.320 --> 07:07.440]  remarkably little data, to make the model more useful. So RLHF is how we align the model to\n",
            "[07:07.520 --> 07:14.080]  what humans want it to do. So there's a giant language model that's trained in a giant data set\n",
            "[07:14.080 --> 07:18.080]  to create this kind of background wisdom knowledge that's contained within the internet.\n",
            "[07:19.280 --> 07:25.840]  And then somehow adding a little bit of human guidance on top of it through this process\n",
            "[07:26.880 --> 07:32.320]  makes it seem so much more awesome. Maybe just because it's much easier to use,\n",
            "[07:32.320 --> 07:35.600]  it's much easier to get what you want. You get it right more often the first time,\n",
            "[07:35.600 --> 07:39.680]  and ease of use matters a lot, even if the base capability was there before.\n",
            "[07:40.240 --> 07:47.840]  And like a feeling like it understood the question you were asking, or like it feels like you're\n",
            "[07:47.840 --> 07:51.840]  kind of on the same page. It's trying to help you. It's the feeling of alignment.\n",
            "[07:51.840 --> 07:56.480]  Yes. I mean, that could be a more technical term for it. And you're saying that not much\n",
            "[07:56.480 --> 07:59.600]  data is required for that, not much human supervision is required for that.\n",
            "[07:59.600 --> 08:07.040]  To be fair, we understand the science of this part at a much earlier stage than we do the science\n",
            "[08:07.040 --> 08:11.440]  of creating these large pre-trained models in the first place. But yes, less data, much less data.\n",
            "[08:11.440 --> 08:15.680]  That's so interesting. The science of human guidance.\n",
            "[08:18.000 --> 08:22.000]  That's a very interesting science. That's going to be a very important science to understand\n",
            "[08:22.720 --> 08:28.880]  how to make it usable, how to make it wise, how to make it ethical, how to make it aligned in\n",
            "[08:28.880 --> 08:31.040]  terms of all the kind of stuff we think about.\n",
            "[08:34.240 --> 08:37.840]  And it matters which are the humans and what is the process of incorporating that human\n",
            "[08:37.840 --> 08:42.240]  feedback. And what are you asking the humans? Is it two things? Are you asking them to rank things?\n",
            "[08:42.240 --> 08:48.160]  What aspects are you letting or asking the humans to focus in on? It's really fascinating.\n",
            "[08:48.160 --> 08:55.840]  But what is the data set it's trained on? Can you kind of loosely speak to the\n",
            "[08:55.920 --> 08:59.120]  enormity of this data set? The pre-training data set. The pre-training data set.\n",
            "[09:00.240 --> 09:03.760]  We spend a huge amount of effort pulling that together from many different sources.\n",
            "[09:04.480 --> 09:11.440]  There's like a lot of open source databases of information. We get stuff via partnerships.\n",
            "[09:11.440 --> 09:15.920]  There's things on the internet. A lot of our work is building a great data set.\n",
            "[09:17.040 --> 09:22.160]  How much of it is the memes subreddit? Not very much. Maybe it'd be more fun if it were more.\n",
            "[09:23.120 --> 09:28.560]  So some of it is Reddit. Some of it is news sources. There's a huge number of newspapers.\n",
            "[09:29.200 --> 09:33.840]  There's the general web. There's a lot of content in the world. More than I think most people think.\n",
            "[09:34.400 --> 09:41.680]  Yeah. There is too much where the task is not to find stuff but to filter out.\n",
            "[09:43.600 --> 09:47.680]  Is there a magic to that? There seems to be several components to solve.\n",
            "[09:48.320 --> 09:54.720]  The design of the algorithm. So like the architecture of the neural networks.\n",
            "[09:54.720 --> 09:58.000]  Maybe the size of the neural network. There's the selection of the data.\n",
            "[09:59.120 --> 10:05.360]  There's the human supervised aspect of it with RL with human feedback.\n",
            "[10:06.080 --> 10:10.880]  Yeah. I think one thing that is not that well understood about creation of this final product,\n",
            "[10:10.880 --> 10:15.360]  like what it takes to make GPT for the version of it we actually ship out and\n",
            "[10:15.360 --> 10:21.440]  that you get to use inside of chat GPT. The number of pieces that have to all come together\n",
            "[10:21.440 --> 10:25.840]  and then we have to figure out either new ideas or just executing existing ideas really well\n",
            "[10:26.240 --> 10:30.640]  at every stage of this pipeline. There's quite a lot that goes into it.\n",
            "[10:30.640 --> 10:36.560]  So there's a lot of problem solving. You've already said for GPT 4 in the blog post\n",
            "[10:36.560 --> 10:43.200]  and in general there's already a maturity that's happening on some of these steps.\n",
            "[10:43.200 --> 10:48.480]  Like being able to predict before doing the full training of how the model will behave.\n",
            "[10:48.480 --> 10:54.320]  Isn't that so remarkable by the way that there's a law of science that lets you predict for these\n",
            "[10:54.320 --> 10:58.960]  inputs. Here's what's going to come out the other end. Here's the level of intelligence you can\n",
            "[10:58.960 --> 11:05.760]  expect. Is it close to science or is it still because you said the word law in science\n",
            "[11:06.480 --> 11:11.600]  which are very ambitious terms close to it. I'd be accurate. Yes.\n",
            "[11:11.600 --> 11:15.600]  I'll say it's way more scientific than I ever would have dared to imagine.\n",
            "[11:15.600 --> 11:22.960]  So you can really know the peculiar characteristics of the fully trained system from just a little\n",
            "[11:22.960 --> 11:27.440]  bit of training. You know like any new branch of science there's we're going to discover new\n",
            "[11:27.440 --> 11:30.160]  things that don't fit the data and have to come up with better explanations and\n",
            "[11:30.800 --> 11:35.280]  you know that is the ongoing process of discovering science. But with what we know now\n",
            "[11:35.840 --> 11:41.040]  even when we had in that GPT 4 blog post like I think we should all just like be in awe of how\n",
            "[11:41.040 --> 11:44.320]  amazing it is that we can even predict to this current level.\n",
            "[11:44.320 --> 11:49.520]  Yeah. You can look at a one year old baby and predict how it's going to do on the SATs.\n",
            "[11:49.520 --> 11:56.080]  I don't know. Seemingly an equivalent one but because here we can actually in detail introspect\n",
            "[11:56.080 --> 12:01.520]  various aspects of the system you can predict. That said just to jump around you said\n",
            "[12:02.480 --> 12:07.280]  the language model that is GPT 4 it learns and quotes something.\n",
            "[12:08.000 --> 12:15.280]  In terms of science and art and so on is there within OpenAI within like folks like yourself\n",
            "[12:15.280 --> 12:21.200]  and Ilias iskiva and the engineers a deeper and deeper understanding of what that something is\n",
            "[12:22.160 --> 12:26.480]  or is it still a kind of beautiful magical mystery.\n",
            "[12:27.840 --> 12:33.040]  Well there's all these different emails that we could talk about and what's an eval.\n",
            "[12:33.120 --> 12:38.880]  Oh like how we measure a model as we're training it after we've trained it and say like you know\n",
            "[12:38.880 --> 12:42.960]  how good is this at some set of tasks. And also just on a small tangent thank you for sort of\n",
            "[12:42.960 --> 12:47.520]  opening sourcing the evaluation process. Yeah I think that'll be really helpful.\n",
            "[12:50.320 --> 12:56.720]  But the one that really matters is we pour all of this effort and money and time into this thing\n",
            "[12:57.280 --> 13:02.320]  and then what it comes out with like how useful is that to people. How much delight does that bring\n",
            "[13:02.320 --> 13:06.880]  people how much does that help them create a much better world new science new products new\n",
            "[13:06.880 --> 13:15.200]  services whatever. And that's the one that matters and understanding for a particular set of inputs\n",
            "[13:15.200 --> 13:20.560]  like how much value and utility to provide to people. I think we are understanding that better.\n",
            "[13:23.840 --> 13:28.400]  Do we understand everything about why the model does one thing and not one other thing?\n",
            "[13:28.400 --> 13:36.480]  Certainly not always but I would say we are pushing back like the fog of war more and more\n",
            "[13:36.480 --> 13:41.200]  and we are you know it took a lot of understanding to make GPT-4 for example.\n",
            "[13:41.760 --> 13:46.480]  But I'm not even sure we can ever fully understand like you said you would understand by asking\n",
            "[13:46.480 --> 13:52.800]  questions essentially because it's compressing all of the web like a huge sloth of the web\n",
            "[13:52.880 --> 14:00.080]  into a small number of parameters into one organized black box that is human wisdom.\n",
            "[14:01.120 --> 14:06.320]  What is that? Human knowledge let's say. Human knowledge. It's a good difference.\n",
            "[14:08.240 --> 14:12.160]  Is there a difference between knowledge? There's so there's facts and there's wisdom and I feel\n",
            "[14:12.160 --> 14:16.720]  like GPT-4 can be also full of wisdom. What's the leap from facts to wisdom?\n",
            "[14:16.720 --> 14:22.720]  You know a funny thing about the way we're training these models is I suspect too much\n",
            "[14:22.720 --> 14:29.680]  of the processing power for lack of a better word is going into using the models of database\n",
            "[14:29.680 --> 14:34.080]  instead of using the model as a reasoning engine. The thing that's really amazing about this\n",
            "[14:34.080 --> 14:38.400]  system is that for some definition of reasoning and we could of course quibble about it and there's\n",
            "[14:38.400 --> 14:43.600]  plenty for which definitions this wouldn't be accurate but for some definition it can do some\n",
            "[14:43.600 --> 14:48.480]  kind of reasoning and you know maybe like the scholars and the experts and like the armchair\n",
            "[14:48.480 --> 14:53.280]  quarterbacks on Twitter would say no it can't you're misusing the word you know whatever whatever\n",
            "[14:53.280 --> 14:58.240]  but I think most people who have used this system would say okay it's doing something in this direction\n",
            "[14:58.960 --> 15:08.080]  and I think that's remarkable and the thing that's most exciting and somehow out of\n",
            "[15:09.680 --> 15:16.080]  ingesting human knowledge it's coming up with this reasoning and capability however we're going\n",
            "[15:16.160 --> 15:23.600]  to talk about that. Now in some senses I think that will be additive to human wisdom and in some\n",
            "[15:23.600 --> 15:28.240]  other senses you can use GPT-4 for all kinds of things and say that appears that there's no wisdom\n",
            "[15:28.240 --> 15:34.320]  in here whatsoever. Yeah at least in interaction with humans it seems to possess wisdom especially\n",
            "[15:34.320 --> 15:41.440]  when there's a continuous interaction of multiple problems so I think what on the Chad GPT site it says\n",
            "[15:42.080 --> 15:50.240]  the dialogue format makes it possible for Chad GPT to answer follow-up questions admit its mistakes\n",
            "[15:50.240 --> 15:56.000]  challenge incorrect premises and reject inappropriate requests but also there's a feeling like it's\n",
            "[15:56.000 --> 16:01.760]  struggling with ideas. Yeah it's always tempting to anthropomorphize this stuff too much but I also\n",
            "[16:01.760 --> 16:08.080]  feel that way. Maybe I'll take a small tangent towards Jordan Peterson who posted on Twitter\n",
            "[16:09.040 --> 16:15.600]  this kind of political question everyone has a different question than when I asked Chad GPT first\n",
            "[16:15.600 --> 16:21.680]  right like the different directions you want to try the dark thing. It somehow says a lot about\n",
            "[16:21.680 --> 16:28.480]  people the first thing. Oh no oh no we don't we don't have to reveal what I asked. We do not.\n",
            "[16:29.280 --> 16:37.120]  I of course ask mathematical questions I've never asked anything dark but Jordan asked it to say\n",
            "[16:37.200 --> 16:43.280]  positive things about the current president Joe Biden and previous president Donald Trump and then\n",
            "[16:44.880 --> 16:52.240]  he asked GPT as a follow-up to say how many characters how long is the string that you generated and he\n",
            "[16:52.240 --> 16:58.240]  showed that the response that contained positive things about Biden was much longer or longer than\n",
            "[16:59.120 --> 17:04.880]  that about Trump and Jordan asked the system to can you rewrite it with an equal number\n",
            "[17:04.880 --> 17:10.560]  equal length string which all of this is just remarkable to me that it understood but it failed\n",
            "[17:10.560 --> 17:22.240]  to do it and it was interested in GPT Chad GPT I think that was 3.5 based was kind of introspective\n",
            "[17:22.240 --> 17:31.760]  about yeah it seems like I failed to do the job correctly and Jordan framed it as Chad GPT was\n",
            "[17:31.760 --> 17:39.280]  lying and aware that it's lying but that framing that's a human anthropomorphization I think\n",
            "[17:40.720 --> 17:47.200]  but that that kind of yeah there there seemed to be a struggle within GPT to understand\n",
            "[17:50.080 --> 17:55.840]  how to do like what it means to generate a text of the same length\n",
            "[17:56.800 --> 18:04.560]  in an answer to a question and also in a sequence of prompts how to understand that it failed to do\n",
            "[18:04.560 --> 18:10.880]  so previously and where it succeeded and all of those like multi like parallel reasonings that\n",
            "[18:10.880 --> 18:16.640]  is doing it just seems like it's struggling so two separate things going on here number one some\n",
            "[18:16.640 --> 18:22.000]  of the things that seem like they should be obvious and easy these models really struggle with yeah\n",
            "[18:22.000 --> 18:25.840]  so I haven't seen this particular example but counting characters counting words that sort of\n",
            "[18:25.840 --> 18:31.120]  stuff that is hard for these models to do well the way they're architected that won't be very accurate\n",
            "[18:32.160 --> 18:38.160]  second we are building in public and we are putting out technology because we think it is\n",
            "[18:38.160 --> 18:43.040]  important for the world to get access to this early to shape the way it's going to be developed to\n",
            "[18:43.040 --> 18:47.360]  help us find the good things and the bad things and every time we put out a new model and we've\n",
            "[18:47.440 --> 18:52.240]  just really felt this with GPT for this week the collective intelligence and ability of the\n",
            "[18:52.240 --> 18:58.000]  outside world helps us discover things we cannot imagine we could have never done internally and\n",
            "[18:58.560 --> 19:03.120]  both like great things that the model can do new capabilities and real weaknesses we have to fix\n",
            "[19:03.120 --> 19:10.160]  and so this iterative process of putting things out finding the the the great parts the bad parts\n",
            "[19:10.160 --> 19:15.920]  improving them quickly and giving people time to feel the technology and shape it with us\n",
            "[19:15.920 --> 19:21.360]  and provide feedback we believe is really important the trade-off of that is the trade-off of building\n",
            "[19:21.360 --> 19:25.680]  in public which is we put out things that are going to be deeply imperfect we want to make our\n",
            "[19:25.680 --> 19:32.640]  mistakes while the stakes are low we want to get it better and better each rep but the like the\n",
            "[19:32.640 --> 19:38.240]  bias of chat GPT when it launched with 3.5 was not something that I certainly felt proud of\n",
            "[19:39.040 --> 19:43.040]  it's gotten much better with GPT for many of the critics and I really respect this have said hey\n",
            "[19:43.040 --> 19:49.360]  a lot of the problems that I had with 3.5 are much better in four but also no two people are ever\n",
            "[19:49.360 --> 19:55.040]  going to agree that one single model is unbiased on every topic and I think the answer there is\n",
            "[19:55.040 --> 20:02.000]  just going to be to give users more personalized control granular control over time and I should\n",
            "[20:02.000 --> 20:09.840]  say in this point yeah I've gotten to know Jordan Peterson and I tried to talk to GPT for about\n",
            "[20:09.920 --> 20:17.840]  Jordan Peterson and I asked it if Jordan Peterson is the fascist first of all it gave context\n",
            "[20:17.840 --> 20:22.720]  it described actual like description of who Jordan Peterson is his career psychologist and so on\n",
            "[20:23.360 --> 20:32.640]  it stated that some number of people have called Jordan Peterson the fascist but there is no\n",
            "[20:32.640 --> 20:38.480]  factual grounding to those claims and it described a bunch of stuff that Jordan believes like he's\n",
            "[20:38.480 --> 20:46.800]  been an outspoken critic of various totalitarian ideologies and he believes in\n",
            "[20:49.520 --> 20:58.800]  individualism and various freedoms that contradict the ideology of fascism and so on and it goes on\n",
            "[20:58.800 --> 21:04.400]  and on like really nicely and it wraps it up it's like a it's a college essay I was like damn one thing\n",
            "[21:04.400 --> 21:10.640]  that I hope these models can do is bring some nuance back to the world yes it felt it felt\n",
            "[21:10.640 --> 21:15.600]  really nuanced you know Twitter kind of destroyed some and maybe we can get some back now that really\n",
            "[21:15.600 --> 21:24.400]  is exciting like for example I asked of course you know did did the covid virus leak from a lab\n",
            "[21:24.400 --> 21:31.760]  again answer very nuanced there's two hypotheses it like describe them it described the the amount\n",
            "[21:31.760 --> 21:37.440]  of data that's available for each it was like it was like a breath of fresh hair when I was a\n",
            "[21:37.440 --> 21:41.200]  little kid I thought building AI we didn't really call it AGI at the time I thought building\n",
            "[21:41.200 --> 21:44.880]  a happy like the coolest thing ever I never really thought I would get the chance to work on it\n",
            "[21:44.880 --> 21:49.360]  but if you had told me that not only I would get the chance to work on it but that after making\n",
            "[21:49.360 --> 21:56.640]  like a very very larval proto AGI thing that the thing I'd have to spend my time on is you know\n",
            "[21:56.640 --> 22:00.640]  trying to like argue with people about whether the number of characters it said nice things about\n",
            "[22:00.640 --> 22:04.880]  one person was different than the number of characters it said nice about some other person\n",
            "[22:04.880 --> 22:08.800]  if you hand people an AGI and that's what they want to do I wouldn't have believed you but I\n",
            "[22:08.800 --> 22:14.560]  understand him one now and I do have empathy for it so what you're implying in that statement is\n",
            "[22:14.560 --> 22:19.280]  we took such giant leaps and the big stuff that they were complaining or arguing about small stuff\n",
            "[22:19.280 --> 22:22.800]  well the small stuff is the big stuff in aggregate so I get it it's just like I\n",
            "[22:24.640 --> 22:30.560]  and I also like I get why this is such an important issue this is a really important issue\n",
            "[22:31.120 --> 22:37.920]  but that somehow we like somehow this is the thing that we get caught up in versus like\n",
            "[22:38.560 --> 22:43.680]  what is this going to mean for our future now maybe you say this is critical to what this is\n",
            "[22:43.680 --> 22:47.280]  going to mean for our future the thing that it says more characters about this person than this\n",
            "[22:47.280 --> 22:51.760]  person and who's deciding that and how it's being decided and how the users get control over that\n",
            "[22:52.480 --> 22:56.720]  maybe that is the most important issue but I wouldn't have guessed it at the time when I was\n",
            "[22:56.720 --> 23:06.160]  like eight-year-old yeah I mean there is and you do there's folks at open AI including yourself\n",
            "[23:06.160 --> 23:11.440]  that do see the importance of these issues to discuss about them under the big banner of AI\n",
            "[23:11.440 --> 23:17.040]  safety that's something that's not often talked about with the release of GPT for how much went into\n",
            "[23:17.040 --> 23:23.280]  the safety concerns how long also you spend on the safety concerns can you can you go through\n",
            "[23:23.280 --> 23:28.800]  some of that process yeah sure what went into AI safety considerations of GPT for release\n",
            "[23:29.360 --> 23:36.960]  so we finished last summer we immediately started giving it to people to to red team\n",
            "[23:37.920 --> 23:42.720]  we started doing a bunch of our own internal safety emails on it we started trying to work\n",
            "[23:42.720 --> 23:50.560]  on different ways to align it and that combination of an internal and external effort plus building\n",
            "[23:50.640 --> 23:55.440]  a whole bunch of new ways to align the model and we didn't get it perfect by far but one thing\n",
            "[23:55.440 --> 24:01.440]  that I care about is that our degree of alignment increases faster than our rate of capability\n",
            "[24:01.440 --> 24:07.760]  progress and that I think will become more and more important over time and I know I think we made\n",
            "[24:07.760 --> 24:12.160]  reasonable progress there to a to a more aligned system than we've ever had before I think this is\n",
            "[24:12.880 --> 24:18.160]  the most capable and most aligned model that we've put out we were able to do a lot of testing on\n",
            "[24:18.160 --> 24:24.720]  it and that takes a while and I totally get why people were like give us GPT for right away\n",
            "[24:26.080 --> 24:31.680]  but I'm happy we did it this way is there some wisdom some insights about that process\n",
            "[24:31.680 --> 24:37.600]  you learned like how to how to solve that problem you can speak to how to solve the like the alignment\n",
            "[24:37.600 --> 24:43.600]  problem so I want to be very clear I do not think we have yet discovered a way to align a super\n",
            "[24:43.600 --> 24:48.480]  powerful system we have we have something that works for our current skill called our LHF\n",
            "[24:49.680 --> 24:57.280]  and we can talk a lot about the benefits of that and the utility it provides it's not just an\n",
            "[24:57.280 --> 25:02.960]  alignment maybe it's not even mostly an alignment capability it helps make a better system a more\n",
            "[25:02.960 --> 25:09.520]  usable system and this is actually something that I don't think people outside the field understand\n",
            "[25:09.520 --> 25:16.000]  enough it's easy to talk about alignment and capability as orthogonal vectors they're very close\n",
            "[25:17.280 --> 25:23.440]  better alignment techniques lead to better capabilities and vice versa there's cases that are different\n",
            "[25:23.440 --> 25:28.960]  and they're important cases but on the whole I think things that you could say like our LHF\n",
            "[25:28.960 --> 25:33.600]  or interpretability that sound like alignment issues also help you make much more capable models\n",
            "[25:34.240 --> 25:40.640]  and the division is just much fuzzier than people think and so in some sense the work we do to\n",
            "[25:40.640 --> 25:45.920]  make GPD4 safer and more aligned looks very similar to all the other work we do of solving\n",
            "[25:45.920 --> 25:54.320]  the research and engineering problems associated with creating useful and powerful models so our\n",
            "[25:54.320 --> 26:00.640]  LHF is the process that came applied very broadly across the entire system where human\n",
            "[26:00.640 --> 26:08.720]  basically votes what's the better way to say something what's you know if a person asks do I\n",
            "[26:08.720 --> 26:15.680]  look fat in this dress there's different ways to ask that question that's aligned with human\n",
            "[26:15.680 --> 26:21.040]  civilization and there's no one set of human values or there's no one set of right answers\n",
            "[26:21.040 --> 26:28.160]  to human civilization so I think what's going to have to happen is we will need to agree on as a\n",
            "[26:28.240 --> 26:33.120]  society on very broad bounds we'll only be able to agree on a very broad bounds of what these\n",
            "[26:33.120 --> 26:38.720]  systems can do and then within those maybe different countries have different RLHF tunes\n",
            "[26:38.720 --> 26:43.760]  certainly individual users have very different preferences we launched this thing with GPD4\n",
            "[26:43.760 --> 26:50.800]  called the system message which is not RLHF but is a way to let users have a good degree of\n",
            "[26:51.600 --> 26:57.920]  steerability over what they want and I think things like that will be important he describes\n",
            "[26:57.920 --> 27:02.640]  as the message and in general how you were able to make GPD4 more steerable\n",
            "[27:05.120 --> 27:09.440]  based on the interaction that the user can have with it which is one of his big really powerful\n",
            "[27:09.440 --> 27:16.720]  things so this system message is a way to say uh you know hey model please pretend like you or\n",
            "[27:16.720 --> 27:25.520]  please only answer this message as if you were Shakespeare doing thing X or please only respond\n",
            "[27:25.520 --> 27:30.560]  with JSON no matter what was one of the examples from our blog post but you could also say any\n",
            "[27:30.560 --> 27:39.840]  number of other things to that and then we we we tuned GPD4 in a way to really treat the system\n",
            "[27:39.840 --> 27:44.880]  message with a lot of authority I'm sure there's jail they'll always not always hopefully but for\n",
            "[27:44.880 --> 27:49.280]  a long time there'll be more jail breaks and we'll keep sort of learning about those but we\n",
            "[27:49.280 --> 27:54.080]  program we develop whatever you want to call it the model in such a way to learn that it's\n",
            "[27:54.080 --> 27:59.280]  supposed to really use that system message can you speak to the kind of the process of writing\n",
            "[27:59.280 --> 28:06.240]  in designing a great prompt as you steer GPD4 I'm not good at this I've met people who are yeah and\n",
            "[28:06.880 --> 28:13.120]  the creativity the kind of they almost some of them almost treated like debugging software\n",
            "[28:13.920 --> 28:21.120]  um but also they they I met people who spend like you know 12 hours a day for a month on end at\n",
            "[28:21.120 --> 28:26.480]  on this and they really get a feel for the model and a feel how different parts of a\n",
            "[28:27.440 --> 28:33.600]  prompt compose with each other like literally the ordering of words this yeah where you put the\n",
            "[28:33.600 --> 28:39.360]  clause when you modify something what kind of word to do it with yeah it's so fascinating because\n",
            "[28:39.440 --> 28:44.080]  like it's remarkable in some sense that's what we do with human conversation right and interacting\n",
            "[28:44.080 --> 28:51.440]  with humans we try to figure out like what words to use to unlock greater wisdom from the other\n",
            "[28:52.480 --> 28:58.240]  the other party the friends of yours are significant others here you get to try it over and over and\n",
            "[28:58.240 --> 29:02.560]  over and over unlimited you could experiment yeah there's all these ways that the kind of\n",
            "[29:02.560 --> 29:08.640]  analogies from humans to a eyes like breakdown and the parallelism the sort of unlimited rollouts\n",
            "[29:08.720 --> 29:14.720]  that's a big one yeah yeah but there's still some parallels that don't break down that there is\n",
            "[29:14.720 --> 29:19.440]  some hundred people here because it's trained on human data there's um it feels like it's a way\n",
            "[29:19.440 --> 29:25.680]  to learn about ourselves by interacting with it some of it as the smarter and smarter gets the\n",
            "[29:25.680 --> 29:33.040]  more represents the more it feels like another human in terms of um the kind of way you would\n",
            "[29:33.040 --> 29:38.800]  phrase the prompt to get the kind of thing you want back and that's interesting because that\n",
            "[29:38.800 --> 29:44.160]  is the art form as you collaborate with it as an assistant this becomes more relevant for\n",
            "[29:44.960 --> 29:48.240]  this is relevant everywhere but it's also very relevant for programming for example\n",
            "[29:48.880 --> 29:55.040]  um I mean just on that topic how do you think gpt4 and all the advancements with gpt change\n",
            "[29:55.040 --> 30:01.040]  the nature of programming today's monday we launched the previous tuesday so it's six days\n",
            "[30:01.520 --> 30:06.160]  the degree while the degree to which it has already changed programming\n",
            "[30:07.920 --> 30:14.560]  and what I have observed from how my friends are creating the tools that are being built on top of\n",
            "[30:14.560 --> 30:23.520]  it um I think this is where we'll see some of the most impact in the short term it's amazing what\n",
            "[30:23.520 --> 30:30.880]  people are doing it's amazing how this tool the leverage it's giving people to do their job\n",
            "[30:30.880 --> 30:36.960]  or their creative work better and better and better it's it's super cool so in the process\n",
            "[30:37.680 --> 30:44.880]  the iterative process you could um ask it to generate a code to do something and then\n",
            "[30:46.720 --> 30:51.520]  the something the code it generates and the something that the code does if you don't like it you can\n",
            "[30:51.520 --> 30:57.200]  ask it to adjust it it's like it's a it's a weird it's a different kind of way of debugging I guess\n",
            "[30:57.200 --> 31:01.040]  for sure the first versions of these systems were sort of you know one shot you sort of you\n",
            "[31:01.040 --> 31:05.440]  said what you wanted it wrote some code and that was it uh now you can have this back and forth\n",
            "[31:05.440 --> 31:09.680]  dialogue where you can say no no I meant this or no no fix this bug or no no do this and then of\n",
            "[31:09.680 --> 31:14.800]  course the next version is the system can debug more on its own and kind of try to like catch\n",
            "[31:14.800 --> 31:23.440]  mistakes as it's making them but this idea of dialogue interfaces and iterating with the computer\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c111ce4b8b4e>\u001b[0m in \u001b[0;36m<cell line: 102>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"### {video_path_local}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     video_transcription = whisper.transcribe(\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mwhisper_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path_local\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\u001b[0m in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, **decode_options)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mdecode_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprompt_reset_since\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDecodingResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_with_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_segment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\u001b[0m in \u001b[0;36mdecode_with_fallback\u001b[0;34m(segment)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecodingOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mdecode_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mneeds_fallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(model, mel, options, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecodingTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msingle\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, mel)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;31m# call the main sampling loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_logprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_speech_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_main_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;31m# reshape the tensors to have (n_audio, n_group) as the first two dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36m_main_loop\u001b[0;34m(self, audio_features, tokens)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36mlogits\u001b[0;34m(self, tokens, audio_features)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcleanup_caching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, xa, kv_cache)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mkv_cache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     ):\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_ln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attn_ln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m# hooks, if installed (i.e. kv_cache is not None), will prepend the cached kv tensors;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m# otherwise, perform key/value projections for self- or cross-attention as usual.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mxa\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mxa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mxa\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mxa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m                     \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1547\u001b[0;31m                     \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36msave_to_cache\u001b[0;34m(module, _, output)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
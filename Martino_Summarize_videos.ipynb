{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinopiaggi/summarize/blob/main/Martino_Summarize_videos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Source of the summary**\n",
        "\n",
        "Type = \"Youtube video or playlist\" #@param ['Text', 'Text from Google Drive','Youtube video or playlist', 'Videos on Google Drive folder']\n",
        "#@markdown ---\n",
        "#@markdown #### **Text (only if Type is Text)**\n",
        "Text = \"Making your bed in the morning, that first cup of coffee, grabbing a croissant on your way to work, listening to your favorite podcast on the train\\u2026 As much as we wish for each day to be different, repeating some of the same actions is an important part of our lives.  Researchers have found that no more than 40% of our actions are consciously self-selected. Instead, we perform these actions in an automated way, without conscious awareness. How can you ramp up that percentage and live a more intentional life?  The key is to understand the difference between habits, routines, and rituals, and to design a life where your daily actions allow you to play with the entire spectrum of consciousness.  Shades of Consciousness Waking up, commuting, walking past a particular store, or starting a meeting at work are common cues that can trigger actions as diverse as smoking a cigarette, buying a croissant, or drinking coffee.  Many books have been written about building better habits and breaking bad ones. The most famous is probably Atomic Habits by James Clear (2018), but other ones such as The Seven Habits of Highly Effective People by Stephen Covey (1989) and The Power of Habits by Charles Duhigg (2012) have also sold millions of copies. It\\u2019s fair to say that people are convinced habits matter.  And it makes sense. To maintain a healthy lifestyle, it helps to be able to set some behaviors on autopilot so that you don\\u2019t have to make a conscious effort every single time. Habits are great for those actions.  But most good habits don\\u2019t start as habits. They start as routines. The main difference between habits and routines is how aware and intentional you are. A habit usually manifests itself as an automatic urge to do something, often triggered by a particular cue. The stronger the connection between the trigger and the habit, the more ingrained the habit.  In contrast, routines require deliberate practice. Making your bed in the morning, going to the gym, going for a hike every Sunday, and meditating are all routines that require you to keep consciously practicing them or they eventually die out. Your brain will not go into automatic mode and walk you to the gym for your weekly HIIT class.  Both habits and routines are regular and repeated actions, but habits happen with little or no conscious thought, whereas routines require a higher degree of intention and effort.  With enough time, routines can turn into habits, but you need enough repetitions to create that habit loop:  Cue. Choose a trigger to tell your brain to start the routine you want to turn into a habit. Routine. Execute the routine, ideally starting with a small, actionable chunk. Reward. Do something enjoyable to tell your brain that this particular action is worth performing again in the future. When it comes to healthy habits, automaticity is good. Building a loop is good. But what about the actions where you actually want to make a conscious effort, the ones where you get satisfaction from pushing yourself out of your comfort zone?  How can you nurture and maintain more intentionality \\u2014 in short, break the loop \\u2014 with these actions?  From Routine to Ritual We tend to associate rituals with very specific types of activities: communal rites of worship, rites of passage, commemorative rites\\u2026 Yes, these are rituals, but this is only the narrowest definition of the term.  More broadly, the difference between a routine and a ritual is the mindset behind the action. While routines can be actions that just need to be done\\u2014such as making your bed or taking a shower\\u2014rituals are viewed as more meaningful practices that have a real sense of purpose.  Design your habits, routines, and rituals with the intentionality curve Rituals do not have to be spiritual or religious. What matters is your level of intentionality. With rituals, you are fully engaged with a focus on the experience of the task, rather than its mere completion. You are investing your highest levels of energy and consciousness.  And you can virtually turn any routine into a ritual by becoming more mindful and making mental space for the action. For instance, when you eat, you could practice paying attention to the textures and the way you chew. Research actually shows that mindful eating can indeed improve the flavor of your food, making you feel more satisfied.  Showering can become an opportunity to become mindful of your body and its connection to your mind. Focus on the sensation of the water on your skin and how your thoughts seem to flow more easily. This way, a simple morning routine can become a morning ritual.  Even cleaning the house can be used to become more aware of your body movements and sensations in your muscles and joints. Just look at some of your existing routines and see if any could become more intentional.  The Intentional Life The power of playing with the spectrum of consciousness when performing daily activities is that you don\\u2019t need to carve extra time for a separate mindfulness practice. Yes, there is lots of research showing the benefits of journaling, yoga, and meditation, but sometimes life gets busy.  Turning a daily routine into a daily ritual is an easy way to inject more intentionality into your life, even when you don\\u2019t have much time or energy.  And being aware of your consciousness levels can also help you create better habits. I call this intentional process of scaling up or down your consciousness levels when performing daily actions the Intentionality Curve. Just ask yourself:  What routines do I want to turn into habits by lowering my intentionality? What routines do I want to turn into rituals by increasing my intentionality? Those two simple questions, if you ask them regularly, can help you practice metacognition in a very tangible way and avoid living your life on autopilot. And that\\u2019s an idea worth playing with.\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown #### **Youtube video or playlist**\n",
        "URL = \"https://youtu.be/L_Guz73e6fw\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown #### **Google Drive video, audio (mp4, wav), or folder containing video and/or audio files**\n",
        "video_path = \"Colab Notebooks/transcription/my_video.mp4\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the source**\n",
        "\n"
      ],
      "metadata": {
        "id": "jwROe6WH2lZi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QOu6v4AuX21"
      },
      "source": [
        "Using https://huggingface.co/facebook/bart-large-cnn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ---\n",
        "#@markdown # **Install libraries**\n",
        "#@markdown This cell will take a little while to download several libraries\n",
        "\n",
        "#@markdown ---\n",
        "!pip install transformers\n",
        "!pip install tensorflow\n",
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\",device=0)\n",
        "\n",
        "import re\n",
        "import math\n",
        "\n",
        "if Type == (\"Youtube video or playlist\" or 'Videos on Google Drive folder'):\n",
        "\n",
        "  video_path_local_list = []\n",
        "  !pip install git+https://github.com/openai/whisper.git\n",
        "  !sudo apt update && sudo apt install ffmpeg\n",
        "\n",
        "if Type == \"Youtube video or playlist\":\n",
        "  !pip install yt-dlp\n",
        "  from pathlib import Path\n",
        "  import yt_dlp\n",
        "\n",
        "  ydl_opts = {\n",
        "  'format': 'm4a/bestaudio/best',\n",
        "  'outtmpl': '%(id)s.%(ext)s',\n",
        "  # ℹ️ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n",
        "  'postprocessors': [{  # Extract audio using ffmpeg\n",
        "  'key': 'FFmpegExtractAudio',\n",
        "  'preferredcodec': 'wav',\n",
        "  }]\n",
        "  }\n",
        "\n",
        "  with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    error_code = ydl.download([URL])\n",
        "    list_video_info = [ydl.extract_info(URL, download=False)]\n",
        "\n",
        "  for video_info in list_video_info:\n",
        "    video_path_local_list.append(Path(f\"{video_info['id']}.wav\"))\n",
        "\n",
        "  for video_path_local in video_path_local_list:\n",
        "    if video_path_local.suffix == \".mp4\":\n",
        "        video_path_local = video_path_local.with_suffix(\".wav\")\n",
        "    result  = subprocess.run([\"ffmpeg\", \"-i\", str(video_path_local.with_suffix(\".mp4\")), \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", str(video_path_local)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sJnZWCPc3uOH",
        "outputId": "9ec51a0e-3b04-48b0-f4a0-cc4bda840d38"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-9e3f_4sb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-9e3f_4sb\n",
            "  Resolved https://github.com/openai/whisper.git to commit b38a1f20f4b23f3f3099af2c3e0ca95627276ddf\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (2.0.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (0.56.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (1.23.5)\n",
            "Collecting torch (from openai-whisper==20230918)\n",
            "  Using cached torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (10.1.0)\n",
            "Requirement already satisfied: tiktoken==0.3.3 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (0.3.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230918) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230918) (2.31.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230918) (3.27.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230918) (3.12.4)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230918) (17.0.3)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230918) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230918) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20230918)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20230918)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20230918)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20230918)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20230918)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20230918)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20230918)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20230918)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20230918)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch->openai-whisper==20230918)\n",
            "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20230918)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch (from openai-whisper==20230918)\n",
            "  Using cached torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (11.7.91)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->openai-whisper==20230918) (0.41.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20230918) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20230918) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230918-py3-none-any.whl size=798405 sha256=fa0e235b104f6e5f2d479e9d0ef6d537a44398bf59af5081889497f4a8057b95\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dx69cvj1/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: torch, openai-whisper\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-whisper-20230918 torch-2.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [555 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,274 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,130 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,398 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,330 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,009 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,305 kB]\n",
            "Hit:17 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 8,344 kB in 3s (2,494 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "19 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting yt-dlp\n",
            "  Downloading yt_dlp-2023.10.13-py2.py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mutagen (from yt-dlp)\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex (from yt-dlp)\n",
            "  Downloading pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets (from yt-dlp)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2023.7.22)\n",
            "Collecting brotli (from yt-dlp)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: brotli, websockets, pycryptodomex, mutagen, yt-dlp\n",
            "Successfully installed brotli-1.1.0 mutagen-1.47.0 pycryptodomex-3.19.0 websockets-11.0.3 yt-dlp-2023.10.13\n",
            "[youtube] Extracting URL: https://youtu.be/L_Guz73e6fw\n",
            "[youtube] L_Guz73e6fw: Downloading webpage\n",
            "[youtube] L_Guz73e6fw: Downloading ios player API JSON\n",
            "[youtube] L_Guz73e6fw: Downloading android player API JSON\n",
            "[youtube] L_Guz73e6fw: Downloading m3u8 information\n",
            "[info] L_Guz73e6fw: Downloading 1 format(s): 140\n",
            "[download] Destination: L_Guz73e6fw.m4a\n",
            "[download] 100% of  133.30MiB in 00:00:03 at 34.28MiB/s  \n",
            "[FixupM4a] Correcting container of \"L_Guz73e6fw.m4a\"\n",
            "[ExtractAudio] Destination: L_Guz73e6fw.wav\n",
            "Deleting original file L_Guz73e6fw.m4a (pass -k to keep)\n",
            "[youtube] Extracting URL: https://youtu.be/L_Guz73e6fw\n",
            "[youtube] L_Guz73e6fw: Downloading webpage\n",
            "[youtube] L_Guz73e6fw: Downloading ios player API JSON\n",
            "[youtube] L_Guz73e6fw: Downloading android player API JSON\n",
            "[youtube] L_Guz73e6fw: Downloading m3u8 information\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-2a524d524ea2>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvideo_path_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".mp4\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mvideo_path_local\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_path_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mresult\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ffmpeg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".mp4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-vn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-acodec\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pcm_s16le\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-ar\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"16000\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-ac\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path_local\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'subprocess' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iOTFm1vPAVDh",
        "outputId": "e09d9c2c-0837-40e6-f8b4-08e742e226c4"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "### L_Guz73e6fw.wav",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:00.000 --> 00:03.840]  We have been a misunderstood and badly mocked org for a long time.\n",
            "[00:03.840 --> 00:09.600]  Like when we started, we announced the org at the end of 2015,\n",
            "[00:10.800 --> 00:12.320]  and said we were going to work on AGI.\n",
            "[00:12.320 --> 00:14.320]  Like people thought we were batshit insane.\n",
            "[00:16.720 --> 00:23.760]  I remember at the time, a eminent AI scientist at a large industrial AI lab\n",
            "[00:24.320 --> 00:29.600]  was like DMing individual reporters being like, these people aren't very good,\n",
            "[00:29.600 --> 00:33.040]  and it's ridiculous to talk about AGI, and I can't believe you're giving them time of day,\n",
            "[00:33.040 --> 00:38.160]  and it's like that was the level of like pettiness and rancor in the field that a new group of people\n",
            "[00:38.160 --> 00:43.600]  say we're going to try to build AGI. So open AI and DeepMind was a small collection of folks\n",
            "[00:43.600 --> 00:52.160]  who were brave enough to talk about AGI in the face of mockery. We don't get mocked as much now.\n",
            "[00:53.040 --> 00:54.400]  Don't get mocked as much now.\n",
            "[00:56.720 --> 01:03.920]  The following is a conversation with Sam Altman, CEO of OpenAI, the company behind GPT-4,\n",
            "[01:03.920 --> 01:10.960]  JAD GPT, Dolly, Codex, and many other AI technologies, which both individually and together\n",
            "[01:10.960 --> 01:15.520]  constitute some of the greatest breakthroughs in the history of artificial intelligence,\n",
            "[01:15.520 --> 01:21.760]  computing, and humanity in general. Please allow me to say a few words about the possibilities\n",
            "[01:22.160 --> 01:26.720]  and the dangers of AI in this current moment in the history of human civilization.\n",
            "[01:27.360 --> 01:33.280]  I believe it is a critical moment. We stand on the precipice of fundamental societal transformation,\n",
            "[01:33.280 --> 01:38.640]  where soon nobody knows when, but many including me believe it's within our lifetime.\n",
            "[01:39.280 --> 01:45.680]  The collective intelligence of the human species begins to pale in comparison by many orders of\n",
            "[01:45.760 --> 01:53.840]  magnitude to the general superintelligence in the AI systems we build and deploy at scale.\n",
            "[01:55.200 --> 02:02.320]  This is both exciting and terrifying. It is exciting because of the innumerable applications\n",
            "[02:02.320 --> 02:09.360]  we know and don't yet know that will empower humans to create, to flourish, to escape the\n",
            "[02:09.360 --> 02:16.160]  widespread poverty and suffering that exists in the world today and to succeed in that old,\n",
            "[02:16.160 --> 02:23.440]  all-too-human pursuit of happiness. It is terrifying because of the power that super-intelligent\n",
            "[02:23.440 --> 02:30.960]  AGI wields to destroy human civilization, intentionally or unintentionally. The power\n",
            "[02:30.960 --> 02:39.040]  to suffocate the human spirit in the totalitarian way of George Orwell's 1984 or the pleasure-fueled\n",
            "[02:39.040 --> 02:46.240]  mass hysteria of Brave New World, where as Huxley saw it, people come to love their oppression,\n",
            "[02:46.800 --> 02:55.200]  to adore the technologies that undo their capacities to think. That is why these conversations\n",
            "[02:55.200 --> 03:01.200]  with the leaders, engineers and philosophers, both optimists and cynics, is important now.\n",
            "[03:02.800 --> 03:08.240]  These are not merely technical conversations about AI. These are conversations about power,\n",
            "[03:08.240 --> 03:13.280]  about companies, institutions and political systems that deploy, check and balance this power,\n",
            "[03:14.000 --> 03:20.480]  about distributed economic systems that incentivize the safety and human alignment of this power,\n",
            "[03:21.200 --> 03:27.520]  about the psychology of the engineers and leaders that deploy AGI, and about the history of human\n",
            "[03:27.520 --> 03:37.360]  nature, our capacity for good and evil at scale. I am deeply honored to have gotten to know and\n",
            "[03:37.360 --> 03:43.040]  to have spoken with on and off the mic with many folks who now work at OpenAI, including\n",
            "[03:43.040 --> 03:52.320]  Sam Altman, Greg Brockman, Iliya Sitzgever, Wojczyk, Saramba, Andrei Karpathy, Jakob Pachaki and many\n",
            "[03:52.320 --> 03:58.560]  others. It means the world that Sam has been totally open with me, willing to have multiple\n",
            "[03:58.560 --> 04:04.400]  conversations, including challenging ones, on and off the mic. I will continue to have these\n",
            "[04:04.480 --> 04:10.720]  conversations to both celebrate the incredible accomplishments of the AI community and the\n",
            "[04:10.720 --> 04:16.000]  steel man, the critical perspective on major decisions various companies and leaders make,\n",
            "[04:16.640 --> 04:24.400]  always with the goal of trying to help in my small way. If I fail, I will work hard to improve.\n",
            "[04:25.120 --> 04:31.280]  I love you all. This is the Lex Friedman podcast. To support it, please check out our sponsors in\n",
            "[04:31.280 --> 04:35.520]  the description. And now, dear friends, here's Sam Altman.\n",
            "[04:36.880 --> 04:42.480]  Hi Level. What is GPT for? How does it work and what to use most amazing about it?\n",
            "[04:43.120 --> 04:50.320]  It's a system that we'll look back at and say it was a very early AI, and it's slow, it's buggy,\n",
            "[04:50.960 --> 04:55.440]  it doesn't do a lot of things very well, but neither did the very earliest computers,\n",
            "[04:56.080 --> 05:01.840]  and they still pointed a path to something that was going to be really important in our lives,\n",
            "[05:01.840 --> 05:06.960]  even though it took a few decades to evolve. Do you think this is a pivotal moment? Out of\n",
            "[05:06.960 --> 05:13.200]  all the versions of GPT 50 years from now, when they look back at an early system that was really\n",
            "[05:13.200 --> 05:19.200]  kind of a leap in a Wikipedia page about the history of artificial intelligence, which of the\n",
            "[05:19.200 --> 05:24.080]  GPTs would they put? That is a good question. I sort of think of progress as this continual\n",
            "[05:24.160 --> 05:30.160]  exponential. It's not like we could say here was the moment where AI went from not happening to\n",
            "[05:30.160 --> 05:35.920]  happening, and I'd have a very hard time pinpointing a single thing. I think it's this very continual\n",
            "[05:35.920 --> 05:40.400]  curve. Well, the history books write about GPT one or two or three or four or seven.\n",
            "[05:41.280 --> 05:46.560]  That's for them to decide. I don't really know. I think if I had to pick some moment\n",
            "[05:47.200 --> 05:52.880]  from what we've seen so far, I'd sort of pick chat GPT. It wasn't the underlying model that\n",
            "[05:52.880 --> 05:56.560]  mattered. It was the usability of it, both the RLHF and the interface to it.\n",
            "[05:57.600 --> 06:03.520]  What is chat GPT? What is RLHF reinforcement learning with human feedback? What was that\n",
            "[06:03.520 --> 06:09.680]  little magic ingredient to the dish that made it so much more delicious?\n",
            "[06:10.560 --> 06:17.200]  So we train these models on a lot of text data, and in that process, they learn the underlying\n",
            "[06:18.080 --> 06:24.080]  something about the underlying representations of what's in here or in there. They can do\n",
            "[06:25.040 --> 06:29.840]  amazing things. But when you first play with that base model that we call it after you finish\n",
            "[06:29.840 --> 06:35.840]  training, it can do very well on evals. It can pass tests. It can do a lot of knowledge in there.\n",
            "[06:36.480 --> 06:43.840]  But it's not very useful, or at least it's not easy to use, let's say. And RLHF is how we take\n",
            "[06:43.840 --> 06:49.600]  some human feedback. The simplest version of this is show two outputs, ask which one is better\n",
            "[06:49.600 --> 06:54.720]  than the other, which one the human raiders prefer, and then feed that back into the model\n",
            "[06:54.720 --> 07:00.320]  with reinforcement learning. And that process works remarkably well, with in my opinion,\n",
            "[07:00.320 --> 07:07.440]  remarkably little data, to make the model more useful. So RLHF is how we align the model to\n",
            "[07:07.520 --> 07:14.080]  what humans want it to do. So there's a giant language model that's trained in a giant data set\n",
            "[07:14.080 --> 07:18.080]  to create this kind of background wisdom knowledge that's contained within the internet.\n",
            "[07:19.280 --> 07:25.840]  And then somehow adding a little bit of human guidance on top of it through this process\n",
            "[07:26.880 --> 07:32.320]  makes it seem so much more awesome. Maybe just because it's much easier to use,\n",
            "[07:32.320 --> 07:35.600]  it's much easier to get what you want. You get it right more often the first time,\n",
            "[07:35.600 --> 07:39.680]  and ease of use matters a lot, even if the base capability was there before.\n",
            "[07:40.240 --> 07:47.840]  And like a feeling like it understood the question you were asking, or like it feels like you're\n",
            "[07:47.840 --> 07:51.840]  kind of on the same page. It's trying to help you. It's the feeling of alignment.\n",
            "[07:51.840 --> 07:56.480]  Yes. I mean, that could be a more technical term for it. And you're saying that not much\n",
            "[07:56.480 --> 07:59.600]  data is required for that, not much human supervision is required for that.\n",
            "[07:59.600 --> 08:07.040]  To be fair, we understand the science of this part at a much earlier stage than we do the science\n",
            "[08:07.040 --> 08:11.440]  of creating these large pre-trained models in the first place. But yes, less data, much less data.\n",
            "[08:11.440 --> 08:15.680]  That's so interesting. The science of human guidance.\n",
            "[08:18.000 --> 08:22.000]  That's a very interesting science. That's going to be a very important science to understand\n",
            "[08:22.720 --> 08:28.880]  how to make it usable, how to make it wise, how to make it ethical, how to make it aligned in\n",
            "[08:28.880 --> 08:31.040]  terms of all the kind of stuff we think about.\n",
            "[08:34.240 --> 08:37.840]  And it matters which are the humans and what is the process of incorporating that human\n",
            "[08:37.840 --> 08:42.240]  feedback. And what are you asking the humans? Is it two things? Are you asking them to rank things?\n",
            "[08:42.240 --> 08:48.160]  What aspects are you letting or asking the humans to focus in on? It's really fascinating.\n",
            "[08:48.160 --> 08:55.840]  But what is the data set it's trained on? Can you kind of loosely speak to the\n",
            "[08:55.920 --> 08:59.120]  enormity of this data set? The pre-training data set. The pre-training data set.\n",
            "[09:00.240 --> 09:03.760]  We spend a huge amount of effort pulling that together from many different sources.\n",
            "[09:04.480 --> 09:11.440]  There's like a lot of open source databases of information. We get stuff via partnerships.\n",
            "[09:11.440 --> 09:15.920]  There's things on the internet. A lot of our work is building a great data set.\n",
            "[09:17.040 --> 09:22.160]  How much of it is the memes subreddit? Not very much. Maybe it'd be more fun if it were more.\n",
            "[09:23.120 --> 09:28.560]  So some of it is Reddit. Some of it is news sources. There's a huge number of newspapers.\n",
            "[09:29.200 --> 09:33.840]  There's the general web. There's a lot of content in the world. More than I think most people think.\n",
            "[09:34.400 --> 09:41.680]  Yeah. There is too much where the task is not to find stuff but to filter out.\n",
            "[09:43.600 --> 09:47.680]  Is there a magic to that? There seems to be several components to solve.\n",
            "[09:48.320 --> 09:54.720]  The design of the algorithm. So like the architecture of the neural networks.\n",
            "[09:54.720 --> 09:58.000]  Maybe the size of the neural network. There's the selection of the data.\n",
            "[09:59.120 --> 10:05.360]  There's the human supervised aspect of it with RL with human feedback.\n",
            "[10:06.080 --> 10:10.880]  Yeah. I think one thing that is not that well understood about creation of this final product,\n",
            "[10:10.880 --> 10:15.360]  like what it takes to make GPT for the version of it we actually ship out and\n",
            "[10:15.360 --> 10:21.440]  that you get to use inside of chat GPT. The number of pieces that have to all come together\n",
            "[10:21.440 --> 10:25.840]  and then we have to figure out either new ideas or just executing existing ideas really well\n",
            "[10:26.240 --> 10:30.640]  at every stage of this pipeline. There's quite a lot that goes into it.\n",
            "[10:30.640 --> 10:36.560]  So there's a lot of problem solving. You've already said for GPT 4 in the blog post\n",
            "[10:36.560 --> 10:43.200]  and in general there's already a maturity that's happening on some of these steps.\n",
            "[10:43.200 --> 10:48.480]  Like being able to predict before doing the full training of how the model will behave.\n",
            "[10:48.480 --> 10:54.320]  Isn't that so remarkable by the way that there's a law of science that lets you predict for these\n",
            "[10:54.320 --> 10:58.960]  inputs. Here's what's going to come out the other end. Here's the level of intelligence you can\n",
            "[10:58.960 --> 11:05.760]  expect. Is it close to science or is it still because you said the word law in science\n",
            "[11:06.480 --> 11:11.600]  which are very ambitious terms close to it. I'd be accurate. Yes.\n",
            "[11:11.600 --> 11:15.600]  I'll say it's way more scientific than I ever would have dared to imagine.\n",
            "[11:15.600 --> 11:22.960]  So you can really know the peculiar characteristics of the fully trained system from just a little\n",
            "[11:22.960 --> 11:27.440]  bit of training. You know like any new branch of science there's we're going to discover new\n",
            "[11:27.440 --> 11:30.160]  things that don't fit the data and have to come up with better explanations and\n",
            "[11:30.800 --> 11:35.280]  you know that is the ongoing process of discovering science. But with what we know now\n",
            "[11:35.840 --> 11:41.040]  even when we had in that GPT 4 blog post like I think we should all just like be in awe of how\n",
            "[11:41.040 --> 11:44.320]  amazing it is that we can even predict to this current level.\n",
            "[11:44.320 --> 11:49.520]  Yeah. You can look at a one year old baby and predict how it's going to do on the SATs.\n",
            "[11:49.520 --> 11:56.080]  I don't know. Seemingly an equivalent one but because here we can actually in detail introspect\n",
            "[11:56.080 --> 12:01.520]  various aspects of the system you can predict. That said just to jump around you said\n",
            "[12:02.480 --> 12:07.280]  the language model that is GPT 4 it learns and quotes something.\n",
            "[12:08.000 --> 12:15.280]  In terms of science and art and so on is there within OpenAI within like folks like yourself\n",
            "[12:15.280 --> 12:21.200]  and Ilias iskiva and the engineers a deeper and deeper understanding of what that something is\n",
            "[12:22.160 --> 12:26.480]  or is it still a kind of beautiful magical mystery.\n",
            "[12:27.840 --> 12:33.040]  Well there's all these different emails that we could talk about and what's an eval.\n",
            "[12:33.120 --> 12:38.880]  Oh like how we measure a model as we're training it after we've trained it and say like you know\n",
            "[12:38.880 --> 12:42.960]  how good is this at some set of tasks. And also just on a small tangent thank you for sort of\n",
            "[12:42.960 --> 12:47.520]  opening sourcing the evaluation process. Yeah I think that'll be really helpful.\n",
            "[12:50.320 --> 12:56.720]  But the one that really matters is we pour all of this effort and money and time into this thing\n",
            "[12:57.280 --> 13:02.320]  and then what it comes out with like how useful is that to people. How much delight does that bring\n",
            "[13:02.320 --> 13:06.880]  people how much does that help them create a much better world new science new products new\n",
            "[13:06.880 --> 13:15.200]  services whatever. And that's the one that matters and understanding for a particular set of inputs\n",
            "[13:15.200 --> 13:20.560]  like how much value and utility to provide to people. I think we are understanding that better.\n",
            "[13:23.840 --> 13:28.400]  Do we understand everything about why the model does one thing and not one other thing?\n",
            "[13:28.400 --> 13:36.480]  Certainly not always but I would say we are pushing back like the fog of war more and more\n",
            "[13:36.480 --> 13:41.200]  and we are you know it took a lot of understanding to make GPT-4 for example.\n",
            "[13:41.760 --> 13:46.480]  But I'm not even sure we can ever fully understand like you said you would understand by asking\n",
            "[13:46.480 --> 13:52.800]  questions essentially because it's compressing all of the web like a huge sloth of the web\n",
            "[13:52.880 --> 14:00.080]  into a small number of parameters into one organized black box that is human wisdom.\n",
            "[14:01.120 --> 14:06.320]  What is that? Human knowledge let's say. Human knowledge. It's a good difference.\n",
            "[14:08.240 --> 14:12.160]  Is there a difference between knowledge? There's so there's facts and there's wisdom and I feel\n",
            "[14:12.160 --> 14:16.720]  like GPT-4 can be also full of wisdom. What's the leap from facts to wisdom?\n",
            "[14:16.720 --> 14:22.720]  You know a funny thing about the way we're training these models is I suspect too much\n",
            "[14:22.720 --> 14:29.680]  of the processing power for lack of a better word is going into using the models of database\n",
            "[14:29.680 --> 14:34.080]  instead of using the model as a reasoning engine. The thing that's really amazing about this\n",
            "[14:34.080 --> 14:38.400]  system is that for some definition of reasoning and we could of course quibble about it and there's\n",
            "[14:38.400 --> 14:43.600]  plenty for which definitions this wouldn't be accurate but for some definition it can do some\n",
            "[14:43.600 --> 14:48.480]  kind of reasoning and you know maybe like the scholars and the experts and like the armchair\n",
            "[14:48.480 --> 14:53.280]  quarterbacks on Twitter would say no it can't you're misusing the word you know whatever whatever\n",
            "[14:53.280 --> 14:58.240]  but I think most people who have used this system would say okay it's doing something in this direction\n",
            "[14:58.960 --> 15:08.080]  and I think that's remarkable and the thing that's most exciting and somehow out of\n",
            "[15:09.680 --> 15:16.080]  ingesting human knowledge it's coming up with this reasoning and capability however we're going\n",
            "[15:16.160 --> 15:23.600]  to talk about that. Now in some senses I think that will be additive to human wisdom and in some\n",
            "[15:23.600 --> 15:28.240]  other senses you can use GPT-4 for all kinds of things and say that appears that there's no wisdom\n",
            "[15:28.240 --> 15:34.320]  in here whatsoever. Yeah at least in interaction with humans it seems to possess wisdom especially\n",
            "[15:34.320 --> 15:41.440]  when there's a continuous interaction of multiple problems so I think what on the Chad GPT site it says\n",
            "[15:42.080 --> 15:50.240]  the dialogue format makes it possible for Chad GPT to answer follow-up questions admit its mistakes\n",
            "[15:50.240 --> 15:56.000]  challenge incorrect premises and reject inappropriate requests but also there's a feeling like it's\n",
            "[15:56.000 --> 16:01.760]  struggling with ideas. Yeah it's always tempting to anthropomorphize this stuff too much but I also\n",
            "[16:01.760 --> 16:08.080]  feel that way. Maybe I'll take a small tangent towards Jordan Peterson who posted on Twitter\n",
            "[16:09.040 --> 16:15.600]  this kind of political question everyone has a different question than when I asked Chad GPT first\n",
            "[16:15.600 --> 16:21.680]  right like the different directions you want to try the dark thing. It somehow says a lot about\n",
            "[16:21.680 --> 16:28.480]  people the first thing. Oh no oh no we don't we don't have to reveal what I asked. We do not.\n",
            "[16:29.280 --> 16:37.120]  I of course ask mathematical questions I've never asked anything dark but Jordan asked it to say\n",
            "[16:37.200 --> 16:43.280]  positive things about the current president Joe Biden and previous president Donald Trump and then\n",
            "[16:44.880 --> 16:52.240]  he asked GPT as a follow-up to say how many characters how long is the string that you generated and he\n",
            "[16:52.240 --> 16:58.240]  showed that the response that contained positive things about Biden was much longer or longer than\n",
            "[16:59.120 --> 17:04.880]  that about Trump and Jordan asked the system to can you rewrite it with an equal number\n",
            "[17:04.880 --> 17:10.560]  equal length string which all of this is just remarkable to me that it understood but it failed\n",
            "[17:10.560 --> 17:22.240]  to do it and it was interested in GPT Chad GPT I think that was 3.5 based was kind of introspective\n",
            "[17:22.240 --> 17:31.760]  about yeah it seems like I failed to do the job correctly and Jordan framed it as Chad GPT was\n",
            "[17:31.760 --> 17:39.280]  lying and aware that it's lying but that framing that's a human anthropomorphization I think\n",
            "[17:40.720 --> 17:47.200]  but that that kind of yeah there there seemed to be a struggle within GPT to understand\n",
            "[17:50.080 --> 17:55.840]  how to do like what it means to generate a text of the same length\n",
            "[17:56.800 --> 18:04.560]  in an answer to a question and also in a sequence of prompts how to understand that it failed to do\n",
            "[18:04.560 --> 18:10.880]  so previously and where it succeeded and all of those like multi like parallel reasonings that\n",
            "[18:10.880 --> 18:16.640]  is doing it just seems like it's struggling so two separate things going on here number one some\n",
            "[18:16.640 --> 18:22.000]  of the things that seem like they should be obvious and easy these models really struggle with yeah\n",
            "[18:22.000 --> 18:25.840]  so I haven't seen this particular example but counting characters counting words that sort of\n",
            "[18:25.840 --> 18:31.120]  stuff that is hard for these models to do well the way they're architected that won't be very accurate\n",
            "[18:32.160 --> 18:38.160]  second we are building in public and we are putting out technology because we think it is\n",
            "[18:38.160 --> 18:43.040]  important for the world to get access to this early to shape the way it's going to be developed to\n",
            "[18:43.040 --> 18:47.360]  help us find the good things and the bad things and every time we put out a new model and we've\n",
            "[18:47.440 --> 18:52.240]  just really felt this with GPT for this week the collective intelligence and ability of the\n",
            "[18:52.240 --> 18:58.000]  outside world helps us discover things we cannot imagine we could have never done internally and\n",
            "[18:58.560 --> 19:03.120]  both like great things that the model can do new capabilities and real weaknesses we have to fix\n",
            "[19:03.120 --> 19:10.160]  and so this iterative process of putting things out finding the the the great parts the bad parts\n",
            "[19:10.160 --> 19:15.920]  improving them quickly and giving people time to feel the technology and shape it with us\n",
            "[19:15.920 --> 19:21.360]  and provide feedback we believe is really important the trade-off of that is the trade-off of building\n",
            "[19:21.360 --> 19:25.680]  in public which is we put out things that are going to be deeply imperfect we want to make our\n",
            "[19:25.680 --> 19:32.640]  mistakes while the stakes are low we want to get it better and better each rep but the like the\n",
            "[19:32.640 --> 19:38.240]  bias of chat GPT when it launched with 3.5 was not something that I certainly felt proud of\n",
            "[19:39.040 --> 19:43.040]  it's gotten much better with GPT for many of the critics and I really respect this have said hey\n",
            "[19:43.040 --> 19:49.360]  a lot of the problems that I had with 3.5 are much better in four but also no two people are ever\n",
            "[19:49.360 --> 19:55.040]  going to agree that one single model is unbiased on every topic and I think the answer there is\n",
            "[19:55.040 --> 20:02.000]  just going to be to give users more personalized control granular control over time and I should\n",
            "[20:02.000 --> 20:09.840]  say in this point yeah I've gotten to know Jordan Peterson and I tried to talk to GPT for about\n",
            "[20:09.920 --> 20:17.840]  Jordan Peterson and I asked it if Jordan Peterson is the fascist first of all it gave context\n",
            "[20:17.840 --> 20:22.720]  it described actual like description of who Jordan Peterson is his career psychologist and so on\n",
            "[20:23.360 --> 20:32.640]  it stated that some number of people have called Jordan Peterson the fascist but there is no\n",
            "[20:32.640 --> 20:38.480]  factual grounding to those claims and it described a bunch of stuff that Jordan believes like he's\n",
            "[20:38.480 --> 20:46.800]  been an outspoken critic of various totalitarian ideologies and he believes in\n",
            "[20:49.520 --> 20:58.800]  individualism and various freedoms that contradict the ideology of fascism and so on and it goes on\n",
            "[20:58.800 --> 21:04.400]  and on like really nicely and it wraps it up it's like a it's a college essay I was like damn one thing\n",
            "[21:04.400 --> 21:10.640]  that I hope these models can do is bring some nuance back to the world yes it felt it felt\n",
            "[21:10.640 --> 21:15.600]  really nuanced you know Twitter kind of destroyed some and maybe we can get some back now that really\n",
            "[21:15.600 --> 21:24.400]  is exciting like for example I asked of course you know did did the covid virus leak from a lab\n",
            "[21:24.400 --> 21:31.760]  again answer very nuanced there's two hypotheses it like describe them it described the the amount\n",
            "[21:31.760 --> 21:37.440]  of data that's available for each it was like it was like a breath of fresh hair when I was a\n",
            "[21:37.440 --> 21:41.200]  little kid I thought building AI we didn't really call it AGI at the time I thought building\n",
            "[21:41.200 --> 21:44.880]  a happy like the coolest thing ever I never really thought I would get the chance to work on it\n",
            "[21:44.880 --> 21:49.360]  but if you had told me that not only I would get the chance to work on it but that after making\n",
            "[21:49.360 --> 21:56.640]  like a very very larval proto AGI thing that the thing I'd have to spend my time on is you know\n",
            "[21:56.640 --> 22:00.640]  trying to like argue with people about whether the number of characters it said nice things about\n",
            "[22:00.640 --> 22:04.880]  one person was different than the number of characters it said nice about some other person\n",
            "[22:04.880 --> 22:08.800]  if you hand people an AGI and that's what they want to do I wouldn't have believed you but I\n",
            "[22:08.800 --> 22:14.560]  understand him one now and I do have empathy for it so what you're implying in that statement is\n",
            "[22:14.560 --> 22:19.280]  we took such giant leaps and the big stuff that they were complaining or arguing about small stuff\n",
            "[22:19.280 --> 22:22.800]  well the small stuff is the big stuff in aggregate so I get it it's just like I\n",
            "[22:24.640 --> 22:30.560]  and I also like I get why this is such an important issue this is a really important issue\n",
            "[22:31.120 --> 22:37.920]  but that somehow we like somehow this is the thing that we get caught up in versus like\n",
            "[22:38.560 --> 22:43.680]  what is this going to mean for our future now maybe you say this is critical to what this is\n",
            "[22:43.680 --> 22:47.280]  going to mean for our future the thing that it says more characters about this person than this\n",
            "[22:47.280 --> 22:51.760]  person and who's deciding that and how it's being decided and how the users get control over that\n",
            "[22:52.480 --> 22:56.720]  maybe that is the most important issue but I wouldn't have guessed it at the time when I was\n",
            "[22:56.720 --> 23:06.160]  like eight-year-old yeah I mean there is and you do there's folks at open AI including yourself\n",
            "[23:06.160 --> 23:11.440]  that do see the importance of these issues to discuss about them under the big banner of AI\n",
            "[23:11.440 --> 23:17.040]  safety that's something that's not often talked about with the release of GPT for how much went into\n",
            "[23:17.040 --> 23:23.280]  the safety concerns how long also you spend on the safety concerns can you can you go through\n",
            "[23:23.280 --> 23:28.800]  some of that process yeah sure what went into AI safety considerations of GPT for release\n",
            "[23:29.360 --> 23:36.960]  so we finished last summer we immediately started giving it to people to to red team\n",
            "[23:37.920 --> 23:42.720]  we started doing a bunch of our own internal safety emails on it we started trying to work\n",
            "[23:42.720 --> 23:50.560]  on different ways to align it and that combination of an internal and external effort plus building\n",
            "[23:50.640 --> 23:55.440]  a whole bunch of new ways to align the model and we didn't get it perfect by far but one thing\n",
            "[23:55.440 --> 24:01.440]  that I care about is that our degree of alignment increases faster than our rate of capability\n",
            "[24:01.440 --> 24:07.760]  progress and that I think will become more and more important over time and I know I think we made\n",
            "[24:07.760 --> 24:12.160]  reasonable progress there to a to a more aligned system than we've ever had before I think this is\n",
            "[24:12.880 --> 24:18.160]  the most capable and most aligned model that we've put out we were able to do a lot of testing on\n",
            "[24:18.160 --> 24:24.720]  it and that takes a while and I totally get why people were like give us GPT for right away\n",
            "[24:26.080 --> 24:31.680]  but I'm happy we did it this way is there some wisdom some insights about that process\n",
            "[24:31.680 --> 24:37.600]  you learned like how to how to solve that problem you can speak to how to solve the like the alignment\n",
            "[24:37.600 --> 24:43.600]  problem so I want to be very clear I do not think we have yet discovered a way to align a super\n",
            "[24:43.600 --> 24:48.480]  powerful system we have we have something that works for our current skill called our LHF\n",
            "[24:49.680 --> 24:57.280]  and we can talk a lot about the benefits of that and the utility it provides it's not just an\n",
            "[24:57.280 --> 25:02.960]  alignment maybe it's not even mostly an alignment capability it helps make a better system a more\n",
            "[25:02.960 --> 25:09.520]  usable system and this is actually something that I don't think people outside the field understand\n",
            "[25:09.520 --> 25:16.000]  enough it's easy to talk about alignment and capability as orthogonal vectors they're very close\n",
            "[25:17.280 --> 25:23.440]  better alignment techniques lead to better capabilities and vice versa there's cases that are different\n",
            "[25:23.440 --> 25:28.960]  and they're important cases but on the whole I think things that you could say like our LHF\n",
            "[25:28.960 --> 25:33.600]  or interpretability that sound like alignment issues also help you make much more capable models\n",
            "[25:34.240 --> 25:40.640]  and the division is just much fuzzier than people think and so in some sense the work we do to\n",
            "[25:40.640 --> 25:45.920]  make GPD4 safer and more aligned looks very similar to all the other work we do of solving\n",
            "[25:45.920 --> 25:54.320]  the research and engineering problems associated with creating useful and powerful models so our\n",
            "[25:54.320 --> 26:00.640]  LHF is the process that came applied very broadly across the entire system where human\n",
            "[26:00.640 --> 26:08.720]  basically votes what's the better way to say something what's you know if a person asks do I\n",
            "[26:08.720 --> 26:15.680]  look fat in this dress there's different ways to ask that question that's aligned with human\n",
            "[26:15.680 --> 26:21.040]  civilization and there's no one set of human values or there's no one set of right answers\n",
            "[26:21.040 --> 26:28.160]  to human civilization so I think what's going to have to happen is we will need to agree on as a\n",
            "[26:28.240 --> 26:33.120]  society on very broad bounds we'll only be able to agree on a very broad bounds of what these\n",
            "[26:33.120 --> 26:38.720]  systems can do and then within those maybe different countries have different RLHF tunes\n",
            "[26:38.720 --> 26:43.760]  certainly individual users have very different preferences we launched this thing with GPD4\n",
            "[26:43.760 --> 26:50.800]  called the system message which is not RLHF but is a way to let users have a good degree of\n",
            "[26:51.600 --> 26:57.920]  steerability over what they want and I think things like that will be important he describes\n",
            "[26:57.920 --> 27:02.640]  as the message and in general how you were able to make GPD4 more steerable\n",
            "[27:05.120 --> 27:09.440]  based on the interaction that the user can have with it which is one of his big really powerful\n",
            "[27:09.440 --> 27:16.720]  things so this system message is a way to say uh you know hey model please pretend like you or\n",
            "[27:16.720 --> 27:25.520]  please only answer this message as if you were Shakespeare doing thing X or please only respond\n",
            "[27:25.520 --> 27:30.560]  with JSON no matter what was one of the examples from our blog post but you could also say any\n",
            "[27:30.560 --> 27:39.840]  number of other things to that and then we we we tuned GPD4 in a way to really treat the system\n",
            "[27:39.840 --> 27:44.880]  message with a lot of authority I'm sure there's jail they'll always not always hopefully but for\n",
            "[27:44.880 --> 27:49.280]  a long time there'll be more jail breaks and we'll keep sort of learning about those but we\n",
            "[27:49.280 --> 27:54.080]  program we develop whatever you want to call it the model in such a way to learn that it's\n",
            "[27:54.080 --> 27:59.280]  supposed to really use that system message can you speak to the kind of the process of writing\n",
            "[27:59.280 --> 28:06.240]  in designing a great prompt as you steer GPD4 I'm not good at this I've met people who are yeah and\n",
            "[28:06.880 --> 28:13.120]  the creativity the kind of they almost some of them almost treated like debugging software\n",
            "[28:13.920 --> 28:21.120]  um but also they they I met people who spend like you know 12 hours a day for a month on end at\n",
            "[28:21.120 --> 28:26.480]  on this and they really get a feel for the model and a feel how different parts of a\n",
            "[28:27.440 --> 28:33.600]  prompt compose with each other like literally the ordering of words this yeah where you put the\n",
            "[28:33.600 --> 28:39.360]  clause when you modify something what kind of word to do it with yeah it's so fascinating because\n",
            "[28:39.440 --> 28:44.080]  like it's remarkable in some sense that's what we do with human conversation right and interacting\n",
            "[28:44.080 --> 28:51.440]  with humans we try to figure out like what words to use to unlock greater wisdom from the other\n",
            "[28:52.480 --> 28:58.240]  the other party the friends of yours are significant others here you get to try it over and over and\n",
            "[28:58.240 --> 29:02.560]  over and over unlimited you could experiment yeah there's all these ways that the kind of\n",
            "[29:02.560 --> 29:08.640]  analogies from humans to a eyes like breakdown and the parallelism the sort of unlimited rollouts\n",
            "[29:08.720 --> 29:14.720]  that's a big one yeah yeah but there's still some parallels that don't break down that there is\n",
            "[29:14.720 --> 29:19.440]  some hundred people here because it's trained on human data there's um it feels like it's a way\n",
            "[29:19.440 --> 29:25.680]  to learn about ourselves by interacting with it some of it as the smarter and smarter gets the\n",
            "[29:25.680 --> 29:33.040]  more represents the more it feels like another human in terms of um the kind of way you would\n",
            "[29:33.040 --> 29:38.800]  phrase the prompt to get the kind of thing you want back and that's interesting because that\n",
            "[29:38.800 --> 29:44.160]  is the art form as you collaborate with it as an assistant this becomes more relevant for\n",
            "[29:44.960 --> 29:48.240]  this is relevant everywhere but it's also very relevant for programming for example\n",
            "[29:48.880 --> 29:55.040]  um I mean just on that topic how do you think gpt4 and all the advancements with gpt change\n",
            "[29:55.040 --> 30:01.040]  the nature of programming today's monday we launched the previous tuesday so it's six days\n",
            "[30:01.520 --> 30:06.160]  the degree while the degree to which it has already changed programming\n",
            "[30:07.920 --> 30:14.560]  and what I have observed from how my friends are creating the tools that are being built on top of\n",
            "[30:14.560 --> 30:23.520]  it um I think this is where we'll see some of the most impact in the short term it's amazing what\n",
            "[30:23.520 --> 30:30.880]  people are doing it's amazing how this tool the leverage it's giving people to do their job\n",
            "[30:30.880 --> 30:36.960]  or their creative work better and better and better it's it's super cool so in the process\n",
            "[30:37.680 --> 30:44.880]  the iterative process you could um ask it to generate a code to do something and then\n",
            "[30:46.720 --> 30:51.520]  the something the code it generates and the something that the code does if you don't like it you can\n",
            "[30:51.520 --> 30:57.200]  ask it to adjust it it's like it's a it's a weird it's a different kind of way of debugging I guess\n",
            "[30:57.200 --> 31:01.040]  for sure the first versions of these systems were sort of you know one shot you sort of you\n",
            "[31:01.040 --> 31:05.440]  said what you wanted it wrote some code and that was it uh now you can have this back and forth\n",
            "[31:05.440 --> 31:09.680]  dialogue where you can say no no I meant this or no no fix this bug or no no do this and then of\n",
            "[31:09.680 --> 31:14.800]  course the next version is the system can debug more on its own and kind of try to like catch\n",
            "[31:14.800 --> 31:23.440]  mistakes as it's making them but this idea of dialogue interfaces and iterating with the computer\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c111ce4b8b4e>\u001b[0m in \u001b[0;36m<cell line: 102>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"### {video_path_local}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     video_transcription = whisper.transcribe(\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mwhisper_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path_local\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\u001b[0m in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, **decode_options)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mdecode_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprompt_reset_since\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDecodingResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_with_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_segment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\u001b[0m in \u001b[0;36mdecode_with_fallback\u001b[0;34m(segment)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecodingOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mdecode_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mneeds_fallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(model, mel, options, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecodingTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msingle\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, mel)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;31m# call the main sampling loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_logprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_speech_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_main_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;31m# reshape the tensors to have (n_audio, n_group) as the first two dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36m_main_loop\u001b[0;34m(self, audio_features, tokens)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36mlogits\u001b[0;34m(self, tokens, audio_features)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcleanup_caching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, xa, kv_cache)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mkv_cache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     ):\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_ln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attn_ln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m# hooks, if installed (i.e. kv_cache is not None), will prepend the cached kv tensors;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m# otherwise, perform key/value projections for self- or cross-attention as usual.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mxa\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mxa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mxa\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mxa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m                     \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1547\u001b[0;31m                     \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36msave_to_cache\u001b[0;34m(module, _, output)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "args = dict(\n",
        "    language = None,\n",
        "    verbose = 'Live transcription',\n",
        "    task = 'transcribe'\n",
        "    temperature = 0.15\n",
        "    temperature_increment_on_fallback = 0.2\n",
        "    best_of = 5\n",
        "    beam_size = 8\n",
        "    patience = 1.0\n",
        "    length_penalty = -0.05\n",
        "    suppress_tokens = \"-1\"\n",
        "    initial_prompt = \"\"\n",
        "    condition_on_previous_text = True\n",
        "    fp16 = True\n",
        "    compression_ratio_threshold = 2.4\n",
        "    logprob_threshold = -1.0\n",
        "    no_speech_threshold = 0.6\n",
        ")\n",
        "\n",
        "\n",
        "for video_path_local in video_path_local_list:\n",
        "    display(Markdown(f\"### {video_path_local}\"))\n",
        "\n",
        "    video_transcription = !whisper \"video_path_local\" --model small\n",
        "    print(video_transcription)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NnXQzbOE_e5",
        "outputId": "4d6ab0d5-972d-4065-bdb4-798a76703cb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1179 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1179\n",
            "Researchers have found that no more than 40% of our actions are consciously self-selected. To maintain a healthy lifestyle, it helps to be able to set some behaviors on autopilot so that you don’t have to make a conscious effort every single time. The key is to understand the difference between habits, routines, and rituals, and to design a life where your daily actions allow you to play with the entire spectrum of consciousness.\n",
            "Turning a daily routine into a daily ritual is an easy way to inject more intentionality into your life. Making your bed in the morning, going to the gym, going for a hike every Sunday, and meditating are all routines that require you to keep consciously practicing them. Rituals do not have to be spiritual or religious. What matters is your level of intentionality. With rituals, you are fully engaged with a focus.  on the experience of the task, rather than its mere completion. The power of playing with the spectrum of consciousness when performing daily activities is that you don’t need to carve out extra time for a separate mindfulness practice, says Dr. David Perry. He calls this intentional process of scaling up or down your consciousness levels the Intentionality Curve.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\",device=0)\n",
        "\n",
        "Text = re.sub(r'\\n|\\r', ' ', Text)\n",
        "Text = re.sub(r' +', ' ', Text)\n",
        "Text = Text.strip()\n",
        "\n",
        "tokenizer = summarizer.tokenizer\n",
        "tokens = tokenizer.encode(Text)\n",
        "\n",
        "print(len(tokens))\n",
        "\n",
        "\n",
        "# Calculate the number of chunks needed\n",
        "chunk_len = math.ceil(len(tokens) / 512)\n",
        "chunksNumber = len(tokens)//chunk_len\n",
        "\n",
        "# Split the tokens into chunks\n",
        "chunks = [tokens[i:i+chunksNumber] for i in range(0, len(tokens), chunksNumber)]\n",
        "\n",
        "if(len(chunks)>1):\n",
        "  if (len(chunks[-1]) + len(chunks[-2])) < 1024:\n",
        "      merged_chunk = chunks.pop(-1) + chunks.pop(-1)\n",
        "      chunks.append(merged_chunk)\n",
        "\n",
        "summary = ''\n",
        "\n",
        "for chunk in chunks:\n",
        "\n",
        "\n",
        "\n",
        "    #print(len(chunk))\n",
        "    # Set max_length and min_length based on token count\n",
        "    max_length = len(chunk) // 2\n",
        "    min_length = len(chunk) // 5\n",
        "\n",
        "    #Generate summary for each chunk without sampling (example)\n",
        "    summary_chunk = summarizer(tokenizer.decode(chunk), max_length=max_length, min_length=min_length, do_sample=True)\n",
        "    summary += summary_chunk[0]['summary_text'] + \"\\n\"\n",
        "\n",
        "print(summary)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOU4edYxsWQp+NSu/OT+OMr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}